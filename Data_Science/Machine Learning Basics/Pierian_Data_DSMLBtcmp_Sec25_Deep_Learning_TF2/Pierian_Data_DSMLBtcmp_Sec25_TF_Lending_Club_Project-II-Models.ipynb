{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeec06",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 Project - Predicting Loan Repayment\n",
    "# Part II: The Models\n",
    "### A. J. Zerouali 2021/09/06\n",
    "\n",
    "* This is the project in Section 25 of Pierian Data's DSML course. It's covered (with solutions) in Lectures 143-151.\n",
    "* This is a continuation of the Lender's Club notebook where the exploratory data analysis is done.\n",
    "\n",
    "## 0) Introduction\n",
    "\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Main question: **Given historical data on loans with information on whether or not the borrower defaulted (charge-off), can we build a model predicting wether or nor a borrower will repay their loan?**\n",
    "\n",
    "This way in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan. Keep in mind classification metrics when evaluating the performance of your model! Our label here is the **\"loan_status\"** column.\n",
    "\n",
    "### Comments and notes\n",
    "\n",
    "1) I'm thinking of doing some hyperparameter optimization for the neural net of this project. The next line is from Google's HParams tutorial:\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams.\n",
    "\n",
    "I would like to avoid doing too much trial and error on my nets in general.\n",
    "\n",
    "2) It would also be useful to paralellize the code. Here's the package to do this in Jupyter: https://ipyparallel.readthedocs.io/en/latest/. To use ipyparallel in Google Colab, have to install it using:\n",
    "           \n",
    "           !pip install ipyparallel\n",
    "           \n",
    "If I absolutely have to work with trial and error, might as well train several nets in parallel.\n",
    "\n",
    "3) I might have to make models that don't include all categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3297846",
   "metadata": {},
   "source": [
    "## 1) Preliminaries\n",
    "\n",
    "### a) Imports\n",
    "\n",
    "* First the obvious imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af4f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b0f16",
   "metadata": {},
   "source": [
    "* Here I'll load the clean data file that I made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lnclb_data = pd.read_csv(\"lenders_club_data_clean_AJZer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3240e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'loan_amnt', 'int_rate', 'installment', 'annual_inc',\n",
       "       'dti', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'mort_acc', 'pub_rec_bankruptcies', 'loan_status_int', 'term_int',\n",
       "       'home_ownership_int', 'verification_status_int', 'purpose_int',\n",
       "       'initial_list_status_int', 'application_type_int', 'emp_length_int'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71aeaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column\n",
    "df_lnclb_data = df_lnclb_data.drop(labels = 'Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d946b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lnclb_info = pd.read_csv(\"lending_club_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588d7c3",
   "metadata": {},
   "source": [
    "* The second csv gives a description of the columns of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7e7438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>loan_status_int</th>\n",
       "      <th>term_int</th>\n",
       "      <th>home_ownership_int</th>\n",
       "      <th>verification_status_int</th>\n",
       "      <th>purpose_int</th>\n",
       "      <th>initial_list_status_int</th>\n",
       "      <th>application_type_int</th>\n",
       "      <th>emp_length_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>11.44</td>\n",
       "      <td>329.48</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>26.24</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36369.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>11.99</td>\n",
       "      <td>265.68</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>22.05</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20131.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>10.49</td>\n",
       "      <td>506.97</td>\n",
       "      <td>43057.0</td>\n",
       "      <td>12.79</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>6.49</td>\n",
       "      <td>220.65</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>17.27</td>\n",
       "      <td>609.33</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>33.95</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24584.0</td>\n",
       "      <td>69.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396025</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10.99</td>\n",
       "      <td>217.38</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>15.63</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>34.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396026</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>12.29</td>\n",
       "      <td>700.42</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>21.45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43263.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396027</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>9.99</td>\n",
       "      <td>161.32</td>\n",
       "      <td>56500.0</td>\n",
       "      <td>17.56</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32704.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396028</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>15.31</td>\n",
       "      <td>503.02</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>15.88</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15704.0</td>\n",
       "      <td>53.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396029</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>13.61</td>\n",
       "      <td>67.98</td>\n",
       "      <td>42996.0</td>\n",
       "      <td>8.32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4292.0</td>\n",
       "      <td>91.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396030 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt  int_rate  installment  annual_inc    dti  open_acc  \\\n",
       "0         10000.0     11.44       329.48    117000.0  26.24      16.0   \n",
       "1          8000.0     11.99       265.68     65000.0  22.05      17.0   \n",
       "2         15600.0     10.49       506.97     43057.0  12.79      13.0   \n",
       "3          7200.0      6.49       220.65     54000.0   2.60       6.0   \n",
       "4         24375.0     17.27       609.33     55000.0  33.95      13.0   \n",
       "...           ...       ...          ...         ...    ...       ...   \n",
       "396025    10000.0     10.99       217.38     40000.0  15.63       6.0   \n",
       "396026    21000.0     12.29       700.42    110000.0  21.45       6.0   \n",
       "396027     5000.0      9.99       161.32     56500.0  17.56      15.0   \n",
       "396028    21000.0     15.31       503.02     64000.0  15.88       9.0   \n",
       "396029     2000.0     13.61        67.98     42996.0   8.32       3.0   \n",
       "\n",
       "        pub_rec  revol_bal  revol_util  total_acc  mort_acc  \\\n",
       "0           0.0    36369.0        41.8       25.0       0.0   \n",
       "1           0.0    20131.0        53.3       27.0       3.0   \n",
       "2           0.0    11987.0        92.2       26.0       0.0   \n",
       "3           0.0     5472.0        21.5       13.0       0.0   \n",
       "4           0.0    24584.0        69.8       43.0       1.0   \n",
       "...         ...        ...         ...        ...       ...   \n",
       "396025      0.0     1990.0        34.3       23.0       0.0   \n",
       "396026      0.0    43263.0        95.7        8.0       1.0   \n",
       "396027      0.0    32704.0        66.9       23.0       0.0   \n",
       "396028      0.0    15704.0        53.8       20.0       5.0   \n",
       "396029      0.0     4292.0        91.3       19.0       1.0   \n",
       "\n",
       "        pub_rec_bankruptcies  loan_status_int  term_int  home_ownership_int  \\\n",
       "0                        0.0                1        36                   1   \n",
       "1                        0.0                1        36                   2   \n",
       "2                        0.0                1        36                   1   \n",
       "3                        0.0                1        36                   1   \n",
       "4                        0.0                0        60                   2   \n",
       "...                      ...              ...       ...                 ...   \n",
       "396025                   0.0                1        60                   1   \n",
       "396026                   0.0                1        36                   2   \n",
       "396027                   0.0                1        36                   1   \n",
       "396028                   0.0                1        60                   2   \n",
       "396029                   0.0                1        36                   1   \n",
       "\n",
       "        verification_status_int  purpose_int  initial_list_status_int  \\\n",
       "0                             0            9                        1   \n",
       "1                             0            0                        0   \n",
       "2                             1            1                        0   \n",
       "3                             0            1                        0   \n",
       "4                             2            1                        0   \n",
       "...                         ...          ...                      ...   \n",
       "396025                        1            0                        1   \n",
       "396026                        1            0                        0   \n",
       "396027                        2            0                        0   \n",
       "396028                        2            0                        0   \n",
       "396029                        2            0                        0   \n",
       "\n",
       "        application_type_int  emp_length_int  \n",
       "0                          0            10.0  \n",
       "1                          0             4.0  \n",
       "2                          0             0.0  \n",
       "3                          0             6.0  \n",
       "4                          0             9.0  \n",
       "...                      ...             ...  \n",
       "396025                     0             2.0  \n",
       "396026                     0             5.0  \n",
       "396027                     0            10.0  \n",
       "396028                     0            10.0  \n",
       "396029                     0            10.0  \n",
       "\n",
       "[396030 rows x 20 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2cce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 396030 entries, 0 to 396029\n",
      "Data columns (total 20 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   loan_amnt                396030 non-null  float64\n",
      " 1   int_rate                 396030 non-null  float64\n",
      " 2   installment              396030 non-null  float64\n",
      " 3   annual_inc               396030 non-null  float64\n",
      " 4   dti                      396030 non-null  float64\n",
      " 5   open_acc                 396030 non-null  float64\n",
      " 6   pub_rec                  396030 non-null  float64\n",
      " 7   revol_bal                396030 non-null  float64\n",
      " 8   revol_util               396030 non-null  float64\n",
      " 9   total_acc                396030 non-null  float64\n",
      " 10  mort_acc                 396030 non-null  float64\n",
      " 11  pub_rec_bankruptcies     396030 non-null  float64\n",
      " 12  loan_status_int          396030 non-null  int64  \n",
      " 13  term_int                 396030 non-null  int64  \n",
      " 14  home_ownership_int       396030 non-null  int64  \n",
      " 15  verification_status_int  396030 non-null  int64  \n",
      " 16  purpose_int              396030 non-null  int64  \n",
      " 17  initial_list_status_int  396030 non-null  int64  \n",
      " 18  application_type_int     396030 non-null  int64  \n",
      " 19  emp_length_int           396030 non-null  float64\n",
      "dtypes: float64(13), int64(7)\n",
      "memory usage: 60.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_lnclb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6224313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    318357\n",
       "0     77673\n",
       "Name: loan_status_int, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data[\"loan_status_int\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce73937",
   "metadata": {},
   "source": [
    "* Next, import the needed data structures and functions from TensorFlow and Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93ab985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sklearn ####\n",
    "# train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Min-max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a30dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tensorflow.keras ####\n",
    "# Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Dense and dopout layers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Callbacks: EarlyStopping and Tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c1214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb025c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fda8262",
   "metadata": {},
   "source": [
    "### b) Data preprocessing\n",
    "\n",
    "* Since we have a large dataset, we'll split it into training, validation and testing. The train_test_split will be for the validation set. The test set will be used for evaluation.\n",
    "* Although I'll make several models, they'll all use the same data obviously.\n",
    "* I'll set the training and validation at 360k entries with 33\\% as validation, and the remainder as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b16353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "X = df_lnclb_data.iloc[:360000].drop(labels=\"loan_status_int\", axis =1).values\n",
    "y = df_lnclb_data[\"loan_status_int\"].iloc[:360000].values\n",
    "\n",
    "# Test data\n",
    "X_Test = df_lnclb_data.iloc[360001:].drop(labels=\"loan_status_int\", axis =1).values\n",
    "y_Test = df_lnclb_data[\"loan_status_int\"].iloc[360001:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ce827",
   "metadata": {},
   "source": [
    "* Apply train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a59f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, X_val, y_train, y_val] = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb9256",
   "metadata": {},
   "source": [
    "* Create the MinMaxScaler and fit to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864bd0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = data_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f83210",
   "metadata": {},
   "source": [
    "* Scale the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2be644c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data_scaler.transform(X_val)\n",
    "X_Test = data_scaler.transform(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df8aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241200, 19)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb285853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118800, 19)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68a3973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36029, 19)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acc550",
   "metadata": {},
   "source": [
    "## 2) Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a1e71",
   "metadata": {},
   "source": [
    "### a) Model 1\n",
    "\n",
    "* For this first model, we'll use all 19 features. Will set dropouts at 0.3.\n",
    "* We'll use training batches of size $2^{14}=16384$.\n",
    "* We'll optimize with ADAM and use the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce7b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential\n",
    "nn_model_01 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model_01.add(Dense(units = 19, activation = \"relu\"))\n",
    "nn_model_01.add(Dropout(rate=0.3))\n",
    "\n",
    "# Hidden layers\n",
    "nn_model_01.add(Dense(units = 10, activation = \"relu\"))\n",
    "nn_model_01.add(Dropout(rate=0.3))\n",
    "\n",
    "nn_model_01.add(Dense(units = 5, activation = \"relu\"))\n",
    "nn_model_01.add(Dropout(rate=0.3))\n",
    "\n",
    "# Output layer with sigmoid activation\n",
    "nn_model_01.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model:\n",
    "nn_model_01.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb1bce",
   "metadata": {},
   "source": [
    "* Create EarlyStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "060604fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_01 = EarlyStopping(monitor='val_loss', patience=50, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b16c4867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241200, 19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f3bc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118800, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9658000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36029, 19)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e845b0",
   "metadata": {},
   "source": [
    "**TRAINING CODE**\n",
    "\n",
    "* I'll skip the board for this first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbf34b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.6792 - val_loss: 0.6252\n",
      "Epoch 2/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.6315 - val_loss: 0.5818\n",
      "Epoch 3/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.6049 - val_loss: 0.5527\n",
      "Epoch 4/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5908 - val_loss: 0.5372\n",
      "Epoch 5/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5809 - val_loss: 0.5289\n",
      "Epoch 6/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5724 - val_loss: 0.5222\n",
      "Epoch 7/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5653 - val_loss: 0.5153\n",
      "Epoch 8/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5585 - val_loss: 0.5093\n",
      "Epoch 9/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5512 - val_loss: 0.5026\n",
      "Epoch 10/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5420 - val_loss: 0.4959\n",
      "Epoch 11/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.5350 - val_loss: 0.4916\n",
      "Epoch 12/800\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5300 - val_loss: 0.4887\n",
      "Epoch 13/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5232 - val_loss: 0.4855\n",
      "Epoch 14/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5206 - val_loss: 0.4836\n",
      "Epoch 15/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5178 - val_loss: 0.4817\n",
      "Epoch 16/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5147 - val_loss: 0.4800\n",
      "Epoch 17/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5130 - val_loss: 0.4783\n",
      "Epoch 18/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5108 - val_loss: 0.4769\n",
      "Epoch 19/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5087 - val_loss: 0.4750\n",
      "Epoch 20/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5078 - val_loss: 0.4740\n",
      "Epoch 21/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5056 - val_loss: 0.4725\n",
      "Epoch 22/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5044 - val_loss: 0.4722\n",
      "Epoch 23/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5031 - val_loss: 0.4705\n",
      "Epoch 24/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5028 - val_loss: 0.4702\n",
      "Epoch 25/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5012 - val_loss: 0.4695\n",
      "Epoch 26/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4994 - val_loss: 0.4683\n",
      "Epoch 27/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4994 - val_loss: 0.4690\n",
      "Epoch 28/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4981 - val_loss: 0.4672\n",
      "Epoch 29/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4978 - val_loss: 0.4672\n",
      "Epoch 30/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4965 - val_loss: 0.4666\n",
      "Epoch 31/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4963 - val_loss: 0.4658\n",
      "Epoch 32/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4954 - val_loss: 0.4666\n",
      "Epoch 33/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4945 - val_loss: 0.4656\n",
      "Epoch 34/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4933 - val_loss: 0.4654\n",
      "Epoch 35/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4929 - val_loss: 0.4650\n",
      "Epoch 36/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4930 - val_loss: 0.4647\n",
      "Epoch 37/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4911 - val_loss: 0.4643\n",
      "Epoch 38/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4921 - val_loss: 0.4640\n",
      "Epoch 39/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4915 - val_loss: 0.4646\n",
      "Epoch 40/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4910 - val_loss: 0.4645\n",
      "Epoch 41/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4896 - val_loss: 0.4630\n",
      "Epoch 42/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4890 - val_loss: 0.4640\n",
      "Epoch 43/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4897 - val_loss: 0.4637\n",
      "Epoch 44/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4890 - val_loss: 0.4635\n",
      "Epoch 45/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4886 - val_loss: 0.4639\n",
      "Epoch 46/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4889 - val_loss: 0.4630\n",
      "Epoch 47/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4875 - val_loss: 0.4633\n",
      "Epoch 48/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4869 - val_loss: 0.4632\n",
      "Epoch 49/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4872 - val_loss: 0.4621\n",
      "Epoch 50/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4864 - val_loss: 0.4630\n",
      "Epoch 51/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4863 - val_loss: 0.4625\n",
      "Epoch 52/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4855 - val_loss: 0.4625\n",
      "Epoch 53/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4853 - val_loss: 0.4617\n",
      "Epoch 54/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4844 - val_loss: 0.4618\n",
      "Epoch 55/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4840 - val_loss: 0.4620\n",
      "Epoch 56/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4836 - val_loss: 0.4616\n",
      "Epoch 57/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4841 - val_loss: 0.4619\n",
      "Epoch 58/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4833 - val_loss: 0.4619\n",
      "Epoch 59/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4826 - val_loss: 0.4614\n",
      "Epoch 60/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4827 - val_loss: 0.4618\n",
      "Epoch 61/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4824 - val_loss: 0.4617\n",
      "Epoch 62/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4821 - val_loss: 0.4612\n",
      "Epoch 63/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4821 - val_loss: 0.4612\n",
      "Epoch 64/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4814 - val_loss: 0.4612\n",
      "Epoch 65/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4814 - val_loss: 0.4616\n",
      "Epoch 66/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4805 - val_loss: 0.4609\n",
      "Epoch 67/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4808 - val_loss: 0.4612\n",
      "Epoch 68/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4807 - val_loss: 0.4611\n",
      "Epoch 69/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4806 - val_loss: 0.4608\n",
      "Epoch 70/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4795 - val_loss: 0.4608\n",
      "Epoch 71/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4793 - val_loss: 0.4609\n",
      "Epoch 72/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4789 - val_loss: 0.4605\n",
      "Epoch 73/800\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4794 - val_loss: 0.4614\n",
      "Epoch 74/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4794 - val_loss: 0.4606\n",
      "Epoch 75/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4784 - val_loss: 0.4609\n",
      "Epoch 76/800\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4784 - val_loss: 0.4604\n",
      "Epoch 77/800\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4774 - val_loss: 0.4606\n",
      "Epoch 78/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4777 - val_loss: 0.4605\n",
      "Epoch 79/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4780 - val_loss: 0.4606\n",
      "Epoch 80/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4776 - val_loss: 0.4609\n",
      "Epoch 81/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4774 - val_loss: 0.4603\n",
      "Epoch 82/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4770 - val_loss: 0.4605\n",
      "Epoch 83/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4764 - val_loss: 0.4603\n",
      "Epoch 84/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4762 - val_loss: 0.4604\n",
      "Epoch 85/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4765 - val_loss: 0.4604\n",
      "Epoch 86/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4763 - val_loss: 0.4602\n",
      "Epoch 87/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4767 - val_loss: 0.4605\n",
      "Epoch 88/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4755 - val_loss: 0.4600\n",
      "Epoch 89/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4756 - val_loss: 0.4603\n",
      "Epoch 90/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4749 - val_loss: 0.4600\n",
      "Epoch 91/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4745 - val_loss: 0.4605\n",
      "Epoch 92/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4746 - val_loss: 0.4601\n",
      "Epoch 93/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4745 - val_loss: 0.4604\n",
      "Epoch 94/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4740 - val_loss: 0.4600\n",
      "Epoch 95/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4743 - val_loss: 0.4600\n",
      "Epoch 96/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4746 - val_loss: 0.4602\n",
      "Epoch 97/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4735 - val_loss: 0.4600\n",
      "Epoch 98/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4737 - val_loss: 0.4603\n",
      "Epoch 99/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4735 - val_loss: 0.4600\n",
      "Epoch 100/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4726 - val_loss: 0.4598\n",
      "Epoch 101/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4732 - val_loss: 0.4603\n",
      "Epoch 102/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4730 - val_loss: 0.4600\n",
      "Epoch 103/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4732 - val_loss: 0.4603\n",
      "Epoch 104/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4726 - val_loss: 0.4599\n",
      "Epoch 105/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4726 - val_loss: 0.4603\n",
      "Epoch 106/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4725 - val_loss: 0.4599\n",
      "Epoch 107/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4721 - val_loss: 0.4597\n",
      "Epoch 108/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4718 - val_loss: 0.4601\n",
      "Epoch 109/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4720 - val_loss: 0.4598\n",
      "Epoch 110/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4717 - val_loss: 0.4601\n",
      "Epoch 111/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4715 - val_loss: 0.4597\n",
      "Epoch 112/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4714 - val_loss: 0.4602\n",
      "Epoch 113/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4713 - val_loss: 0.4597\n",
      "Epoch 114/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4712 - val_loss: 0.4599\n",
      "Epoch 115/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4715 - val_loss: 0.4601\n",
      "Epoch 116/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4710 - val_loss: 0.4597\n",
      "Epoch 117/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4707 - val_loss: 0.4598\n",
      "Epoch 118/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4714 - val_loss: 0.4602\n",
      "Epoch 119/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4709 - val_loss: 0.4598\n",
      "Epoch 120/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4707 - val_loss: 0.4599\n",
      "Epoch 121/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4703 - val_loss: 0.4598\n",
      "Epoch 122/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4704 - val_loss: 0.4600\n",
      "Epoch 123/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4699 - val_loss: 0.4597\n",
      "Epoch 124/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4703 - val_loss: 0.4598\n",
      "Epoch 125/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4701 - val_loss: 0.4599\n",
      "Epoch 126/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4698 - val_loss: 0.4599\n",
      "Epoch 127/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4695 - val_loss: 0.4596\n",
      "Epoch 128/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4696 - val_loss: 0.4597\n",
      "Epoch 129/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4695 - val_loss: 0.4598\n",
      "Epoch 130/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4690 - val_loss: 0.4596\n",
      "Epoch 131/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4694 - val_loss: 0.4598\n",
      "Epoch 132/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4697 - val_loss: 0.4599\n",
      "Epoch 133/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4691 - val_loss: 0.4596\n",
      "Epoch 134/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4689 - val_loss: 0.4597\n",
      "Epoch 135/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4687 - val_loss: 0.4597\n",
      "Epoch 136/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4688 - val_loss: 0.4596\n",
      "Epoch 137/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4689 - val_loss: 0.4595\n",
      "Epoch 138/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4689 - val_loss: 0.4599\n",
      "Epoch 139/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4686 - val_loss: 0.4596\n",
      "Epoch 140/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4688 - val_loss: 0.4599\n",
      "Epoch 141/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4685 - val_loss: 0.4597\n",
      "Epoch 142/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4688 - val_loss: 0.4597\n",
      "Epoch 143/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4682 - val_loss: 0.4596\n",
      "Epoch 144/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4682 - val_loss: 0.4598\n",
      "Epoch 145/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4686 - val_loss: 0.4598\n",
      "Epoch 146/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4680 - val_loss: 0.4596\n",
      "Epoch 147/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4677 - val_loss: 0.4595\n",
      "Epoch 148/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4679 - val_loss: 0.4594\n",
      "Epoch 149/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4673 - val_loss: 0.4595\n",
      "Epoch 150/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4683 - val_loss: 0.4599\n",
      "Epoch 151/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4675 - val_loss: 0.4596\n",
      "Epoch 152/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4677 - val_loss: 0.4594\n",
      "Epoch 153/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4674 - val_loss: 0.4597\n",
      "Epoch 154/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4676 - val_loss: 0.4597\n",
      "Epoch 155/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4670 - val_loss: 0.4593\n",
      "Epoch 156/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4674 - val_loss: 0.4595\n",
      "Epoch 157/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4673 - val_loss: 0.4595\n",
      "Epoch 158/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4671 - val_loss: 0.4596\n",
      "Epoch 159/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4671 - val_loss: 0.4593\n",
      "Epoch 160/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4668 - val_loss: 0.4596\n",
      "Epoch 161/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4668 - val_loss: 0.4596\n",
      "Epoch 162/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4674 - val_loss: 0.4596\n",
      "Epoch 163/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4674 - val_loss: 0.4594\n",
      "Epoch 164/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4664 - val_loss: 0.4591\n",
      "Epoch 165/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4669 - val_loss: 0.4592\n",
      "Epoch 166/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4666 - val_loss: 0.4592\n",
      "Epoch 167/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4668 - val_loss: 0.4592\n",
      "Epoch 168/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4667 - val_loss: 0.4592\n",
      "Epoch 169/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4664 - val_loss: 0.4592\n",
      "Epoch 170/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4665 - val_loss: 0.4592\n",
      "Epoch 171/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4663 - val_loss: 0.4593\n",
      "Epoch 172/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4660 - val_loss: 0.4590\n",
      "Epoch 173/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4664 - val_loss: 0.4592\n",
      "Epoch 174/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4659 - val_loss: 0.4587\n",
      "Epoch 175/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4663 - val_loss: 0.4592\n",
      "Epoch 176/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4661 - val_loss: 0.4590\n",
      "Epoch 177/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4660 - val_loss: 0.4591\n",
      "Epoch 178/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4656 - val_loss: 0.4591\n",
      "Epoch 179/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4658 - val_loss: 0.4588\n",
      "Epoch 180/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4657 - val_loss: 0.4589\n",
      "Epoch 181/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4656 - val_loss: 0.4590\n",
      "Epoch 182/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4656 - val_loss: 0.4588\n",
      "Epoch 183/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4652 - val_loss: 0.4589\n",
      "Epoch 184/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4661 - val_loss: 0.4590\n",
      "Epoch 185/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4654 - val_loss: 0.4587\n",
      "Epoch 186/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4660 - val_loss: 0.4591\n",
      "Epoch 187/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4655 - val_loss: 0.4589\n",
      "Epoch 188/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4654 - val_loss: 0.4589\n",
      "Epoch 189/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4652 - val_loss: 0.4588\n",
      "Epoch 190/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4652 - val_loss: 0.4586\n",
      "Epoch 191/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4656 - val_loss: 0.4591\n",
      "Epoch 192/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4650 - val_loss: 0.4588\n",
      "Epoch 193/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4653 - val_loss: 0.4590\n",
      "Epoch 194/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4653 - val_loss: 0.4587\n",
      "Epoch 195/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4648 - val_loss: 0.4586\n",
      "Epoch 196/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4653 - val_loss: 0.4588\n",
      "Epoch 197/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4651 - val_loss: 0.4586\n",
      "Epoch 198/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4646 - val_loss: 0.4587\n",
      "Epoch 199/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4653 - val_loss: 0.4587\n",
      "Epoch 200/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4651 - val_loss: 0.4590\n",
      "Epoch 201/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4648 - val_loss: 0.4587\n",
      "Epoch 202/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4648 - val_loss: 0.4587\n",
      "Epoch 203/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4650 - val_loss: 0.4587\n",
      "Epoch 204/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4649 - val_loss: 0.4586\n",
      "Epoch 205/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4647 - val_loss: 0.4586\n",
      "Epoch 206/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4644 - val_loss: 0.4584\n",
      "Epoch 207/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4647 - val_loss: 0.4587\n",
      "Epoch 208/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4654 - val_loss: 0.4589\n",
      "Epoch 209/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4646 - val_loss: 0.4584\n",
      "Epoch 210/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4646 - val_loss: 0.4589\n",
      "Epoch 211/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4649 - val_loss: 0.4586\n",
      "Epoch 212/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4649 - val_loss: 0.4588\n",
      "Epoch 213/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4649 - val_loss: 0.4587\n",
      "Epoch 214/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4645 - val_loss: 0.4585\n",
      "Epoch 215/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4647 - val_loss: 0.4587\n",
      "Epoch 216/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4645 - val_loss: 0.4585\n",
      "Epoch 217/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4644 - val_loss: 0.4586\n",
      "Epoch 218/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4648 - val_loss: 0.4587\n",
      "Epoch 219/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4645 - val_loss: 0.4584\n",
      "Epoch 220/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4644 - val_loss: 0.4587\n",
      "Epoch 221/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4641 - val_loss: 0.4586\n",
      "Epoch 222/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4642 - val_loss: 0.4583\n",
      "Epoch 223/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4641 - val_loss: 0.4587\n",
      "Epoch 224/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4641 - val_loss: 0.4583\n",
      "Epoch 225/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4641 - val_loss: 0.4585\n",
      "Epoch 226/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4642 - val_loss: 0.4585\n",
      "Epoch 227/800\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4646 - val_loss: 0.4588\n",
      "Epoch 228/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4642 - val_loss: 0.4583\n",
      "Epoch 229/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4641 - val_loss: 0.4587\n",
      "Epoch 230/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4642 - val_loss: 0.4584\n",
      "Epoch 231/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4640 - val_loss: 0.4583\n",
      "Epoch 232/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4639 - val_loss: 0.4585\n",
      "Epoch 233/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4645 - val_loss: 0.4584\n",
      "Epoch 234/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4639 - val_loss: 0.4581\n",
      "Epoch 235/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4641 - val_loss: 0.4586\n",
      "Epoch 236/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4643 - val_loss: 0.4583\n",
      "Epoch 237/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4637 - val_loss: 0.4582\n",
      "Epoch 238/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4635 - val_loss: 0.4583\n",
      "Epoch 239/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4644 - val_loss: 0.4583\n",
      "Epoch 240/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4640 - val_loss: 0.4584\n",
      "Epoch 241/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4640 - val_loss: 0.4582\n",
      "Epoch 242/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4639 - val_loss: 0.4583\n",
      "Epoch 243/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4642 - val_loss: 0.4582\n",
      "Epoch 244/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4636 - val_loss: 0.4584\n",
      "Epoch 245/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4640 - val_loss: 0.4582\n",
      "Epoch 246/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4583\n",
      "Epoch 247/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4645 - val_loss: 0.4586\n",
      "Epoch 248/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4637 - val_loss: 0.4581\n",
      "Epoch 249/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4639 - val_loss: 0.4586\n",
      "Epoch 250/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4638 - val_loss: 0.4583\n",
      "Epoch 251/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4642 - val_loss: 0.4584\n",
      "Epoch 252/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4640 - val_loss: 0.4583\n",
      "Epoch 253/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4638 - val_loss: 0.4580\n",
      "Epoch 254/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4637 - val_loss: 0.4585\n",
      "Epoch 255/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4635 - val_loss: 0.4579\n",
      "Epoch 256/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4634 - val_loss: 0.4584\n",
      "Epoch 257/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4637 - val_loss: 0.4581\n",
      "Epoch 258/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4635 - val_loss: 0.4581\n",
      "Epoch 259/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4637 - val_loss: 0.4583\n",
      "Epoch 260/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4637 - val_loss: 0.4580\n",
      "Epoch 261/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4633 - val_loss: 0.4585\n",
      "Epoch 262/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4634 - val_loss: 0.4581\n",
      "Epoch 263/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4581\n",
      "Epoch 264/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4631 - val_loss: 0.4579\n",
      "Epoch 265/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4639 - val_loss: 0.4582\n",
      "Epoch 266/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4581\n",
      "Epoch 267/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4629 - val_loss: 0.4582\n",
      "Epoch 268/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4579\n",
      "Epoch 269/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4634 - val_loss: 0.4580\n",
      "Epoch 270/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4638 - val_loss: 0.4584\n",
      "Epoch 271/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4634 - val_loss: 0.4580\n",
      "Epoch 272/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4635 - val_loss: 0.4580\n",
      "Epoch 273/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4583\n",
      "Epoch 274/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4629 - val_loss: 0.4577\n",
      "Epoch 275/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4635 - val_loss: 0.4582\n",
      "Epoch 276/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4639 - val_loss: 0.4583\n",
      "Epoch 277/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4579\n",
      "Epoch 278/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4580\n",
      "Epoch 279/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4582\n",
      "Epoch 280/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4580\n",
      "Epoch 281/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4579\n",
      "Epoch 282/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4582\n",
      "Epoch 283/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4580\n",
      "Epoch 284/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4578\n",
      "Epoch 285/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4634 - val_loss: 0.4582\n",
      "Epoch 286/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4581\n",
      "Epoch 287/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4638 - val_loss: 0.4580\n",
      "Epoch 288/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4632 - val_loss: 0.4580\n",
      "Epoch 289/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4636 - val_loss: 0.4581\n",
      "Epoch 290/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4635 - val_loss: 0.4582\n",
      "Epoch 291/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4579\n",
      "Epoch 292/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4636 - val_loss: 0.4581\n",
      "Epoch 293/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4629 - val_loss: 0.4580\n",
      "Epoch 294/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4631 - val_loss: 0.4580\n",
      "Epoch 295/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4577\n",
      "Epoch 296/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4632 - val_loss: 0.4582\n",
      "Epoch 297/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4628 - val_loss: 0.4580\n",
      "Epoch 298/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4585\n",
      "Epoch 299/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4579\n",
      "Epoch 300/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4630 - val_loss: 0.4578\n",
      "Epoch 301/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4582\n",
      "Epoch 302/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4631 - val_loss: 0.4582\n",
      "Epoch 303/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4578\n",
      "Epoch 304/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4626 - val_loss: 0.4578\n",
      "Epoch 305/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4584\n",
      "Epoch 306/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4576\n",
      "Epoch 307/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4629 - val_loss: 0.4583\n",
      "Epoch 308/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4629 - val_loss: 0.4576\n",
      "Epoch 309/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4630 - val_loss: 0.4582\n",
      "Epoch 310/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4633 - val_loss: 0.4582\n",
      "Epoch 311/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4576\n",
      "Epoch 312/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4581\n",
      "Epoch 313/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4633 - val_loss: 0.4581\n",
      "Epoch 314/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4576\n",
      "Epoch 315/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4632 - val_loss: 0.4581\n",
      "Epoch 316/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4629 - val_loss: 0.4580\n",
      "Epoch 317/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4631 - val_loss: 0.4579\n",
      "Epoch 318/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4630 - val_loss: 0.4581\n",
      "Epoch 319/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4632 - val_loss: 0.4580\n",
      "Epoch 320/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4627 - val_loss: 0.4577\n",
      "Epoch 321/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4581\n",
      "Epoch 322/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4631 - val_loss: 0.4582\n",
      "Epoch 323/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4626 - val_loss: 0.4578\n",
      "Epoch 324/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4579\n",
      "Epoch 325/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4630 - val_loss: 0.4579\n",
      "Epoch 326/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4631 - val_loss: 0.4577\n",
      "Epoch 327/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4576\n",
      "Epoch 328/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4630 - val_loss: 0.4580\n",
      "Epoch 329/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4627 - val_loss: 0.4578\n",
      "Epoch 330/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4626 - val_loss: 0.4582\n",
      "Epoch 331/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4626 - val_loss: 0.4577\n",
      "Epoch 332/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4631 - val_loss: 0.4578\n",
      "Epoch 333/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4630 - val_loss: 0.4580\n",
      "Epoch 334/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4579\n",
      "Epoch 335/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4629 - val_loss: 0.4582\n",
      "Epoch 336/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4630 - val_loss: 0.4580\n",
      "Epoch 337/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4629 - val_loss: 0.4578\n",
      "Epoch 338/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4581\n",
      "Epoch 339/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4583\n",
      "Epoch 340/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4632 - val_loss: 0.4583\n",
      "Epoch 341/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4634 - val_loss: 0.4579\n",
      "Epoch 342/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4630 - val_loss: 0.4585\n",
      "Epoch 343/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4626 - val_loss: 0.4581\n",
      "Epoch 344/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4632 - val_loss: 0.4583\n",
      "Epoch 345/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4622 - val_loss: 0.4580\n",
      "Epoch 346/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 347/800\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.462 - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4578\n",
      "Epoch 348/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4584\n",
      "Epoch 349/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4630 - val_loss: 0.4582\n",
      "Epoch 350/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4575\n",
      "Epoch 351/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4579\n",
      "Epoch 352/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4626 - val_loss: 0.4577\n",
      "Epoch 353/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4630 - val_loss: 0.4579\n",
      "Epoch 354/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4629 - val_loss: 0.4578\n",
      "Epoch 355/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4625 - val_loss: 0.4579\n",
      "Epoch 356/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4626 - val_loss: 0.4577\n",
      "Epoch 357/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4624 - val_loss: 0.4579\n",
      "Epoch 358/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4624 - val_loss: 0.4576\n",
      "Epoch 359/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4626 - val_loss: 0.4577\n",
      "Epoch 360/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4630 - val_loss: 0.4581\n",
      "Epoch 361/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4575\n",
      "Epoch 362/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4577\n",
      "Epoch 363/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4578\n",
      "Epoch 364/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 365/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4627 - val_loss: 0.4576\n",
      "Epoch 366/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4575\n",
      "Epoch 367/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4628 - val_loss: 0.4581\n",
      "Epoch 368/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4577\n",
      "Epoch 369/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4624 - val_loss: 0.4579\n",
      "Epoch 370/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4621 - val_loss: 0.4576\n",
      "Epoch 371/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4578\n",
      "Epoch 372/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4577\n",
      "Epoch 373/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4627 - val_loss: 0.4575\n",
      "Epoch 374/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4620 - val_loss: 0.4577\n",
      "Epoch 375/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4623 - val_loss: 0.4576\n",
      "Epoch 376/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4624 - val_loss: 0.4576\n",
      "Epoch 377/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4581\n",
      "Epoch 378/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4623 - val_loss: 0.4578\n",
      "Epoch 379/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4577\n",
      "Epoch 380/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4627 - val_loss: 0.4575\n",
      "Epoch 381/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4618 - val_loss: 0.4574\n",
      "Epoch 382/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4630 - val_loss: 0.4576\n",
      "Epoch 383/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4581\n",
      "Epoch 384/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4621 - val_loss: 0.4579\n",
      "Epoch 385/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4624 - val_loss: 0.4580\n",
      "Epoch 386/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4626 - val_loss: 0.4579\n",
      "Epoch 387/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4623 - val_loss: 0.4577\n",
      "Epoch 388/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4622 - val_loss: 0.4580\n",
      "Epoch 389/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4619 - val_loss: 0.4576\n",
      "Epoch 390/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4625 - val_loss: 0.4577\n",
      "Epoch 391/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4622 - val_loss: 0.4581\n",
      "Epoch 392/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4622 - val_loss: 0.4579\n",
      "Epoch 393/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4627 - val_loss: 0.4581\n",
      "Epoch 394/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4626 - val_loss: 0.4578\n",
      "Epoch 395/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4624 - val_loss: 0.4571\n",
      "Epoch 396/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4624 - val_loss: 0.4575\n",
      "Epoch 397/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4619 - val_loss: 0.4573\n",
      "Epoch 398/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4623 - val_loss: 0.4574\n",
      "Epoch 399/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4616 - val_loss: 0.4576\n",
      "Epoch 400/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4579\n",
      "Epoch 401/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4625 - val_loss: 0.4584\n",
      "Epoch 402/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4622 - val_loss: 0.4580\n",
      "Epoch 403/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4620 - val_loss: 0.4580\n",
      "Epoch 404/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4621 - val_loss: 0.4579\n",
      "Epoch 405/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4620 - val_loss: 0.4580\n",
      "Epoch 406/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4620 - val_loss: 0.4577\n",
      "Epoch 407/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4620 - val_loss: 0.4579\n",
      "Epoch 408/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4619 - val_loss: 0.4572\n",
      "Epoch 409/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4626 - val_loss: 0.4577\n",
      "Epoch 410/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4620 - val_loss: 0.4581\n",
      "Epoch 411/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4622 - val_loss: 0.4581\n",
      "Epoch 412/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4619 - val_loss: 0.4575\n",
      "Epoch 413/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4619 - val_loss: 0.4576\n",
      "Epoch 414/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4624 - val_loss: 0.4575\n",
      "Epoch 415/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4617 - val_loss: 0.4574\n",
      "Epoch 416/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4621 - val_loss: 0.4582\n",
      "Epoch 417/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4581\n",
      "Epoch 418/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4620 - val_loss: 0.4577\n",
      "Epoch 419/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4619 - val_loss: 0.4578\n",
      "Epoch 420/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4620 - val_loss: 0.4579\n",
      "Epoch 421/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4621 - val_loss: 0.4572\n",
      "Epoch 422/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4619 - val_loss: 0.4572\n",
      "Epoch 423/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4618 - val_loss: 0.4582\n",
      "Epoch 424/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4616 - val_loss: 0.4578\n",
      "Epoch 425/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4624 - val_loss: 0.4583\n",
      "Epoch 426/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4621 - val_loss: 0.4580\n",
      "Epoch 427/800\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4620 - val_loss: 0.4575\n",
      "Epoch 428/800\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4619 - val_loss: 0.4582\n",
      "Epoch 429/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4619 - val_loss: 0.4583\n",
      "Epoch 430/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4620 - val_loss: 0.4583\n",
      "Epoch 431/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4623 - val_loss: 0.4582\n",
      "Epoch 432/800\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4622 - val_loss: 0.4576\n",
      "Epoch 433/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4619 - val_loss: 0.4573\n",
      "Epoch 434/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4615 - val_loss: 0.4573\n",
      "Epoch 435/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4622 - val_loss: 0.4576\n",
      "Epoch 436/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4617 - val_loss: 0.4576\n",
      "Epoch 437/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4618 - val_loss: 0.4580\n",
      "Epoch 438/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4617 - val_loss: 0.4575\n",
      "Epoch 439/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4620 - val_loss: 0.4575\n",
      "Epoch 440/800\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4620 - val_loss: 0.4578\n",
      "Epoch 441/800\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4618 - val_loss: 0.4574\n",
      "Epoch 442/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4618 - val_loss: 0.4572\n",
      "Epoch 443/800\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4616 - val_loss: 0.4575\n",
      "Epoch 444/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4619 - val_loss: 0.4578\n",
      "Epoch 445/800\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4617 - val_loss: 0.4576\n",
      "Epoch 00445: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24489d9f6d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_01.fit(x = X_train, y=y_train, \\\n",
    "                validation_data=(X_val, y_val),\\\n",
    "                batch_size = 16384, validation_batch_size = 16384,\n",
    "                epochs = 800, \\\n",
    "                callbacks=[early_stop_01],\\\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd305c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(nn_model_01.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09b736",
   "metadata": {},
   "source": [
    "* Let's look at the predictions for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53cf0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_01 = (nn_model_01.predict(X_Test) > 0.8).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905204b2",
   "metadata": {},
   "source": [
    "* Let's look at the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59a55cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.55      0.42      7147\n",
      "           1       0.87      0.74      0.80     28882\n",
      "\n",
      "    accuracy                           0.70     36029\n",
      "   macro avg       0.61      0.65      0.61     36029\n",
      "weighted avg       0.77      0.70      0.72     36029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_Test, y_pred=z_pred_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "621f8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3941  3206]\n",
      " [ 7527 21355]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true = y_Test, y_pred=z_pred_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa74ee7",
   "metadata": {},
   "source": [
    "* This is bad. Here are the number of ones in each y vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "451990a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193979"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1570907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95495"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f11e93ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_Test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f86005",
   "metadata": {},
   "source": [
    "* In the test data, there's an overwhelming number of ones. Should I shuffle the data more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2307d",
   "metadata": {},
   "source": [
    "**Comments (21/09/06; 13:00)**\n",
    "* There's a huge problem with this model... It doesn't predict defaults (0's) accurately. I changed the sigmoid probability  threshold from 0.5 to 0.6, 0.7, 0.75, 0.8, 0.85, 0.9. No results are good here.\n",
    "* Save the model for now (21/09/06, 13:10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54b551e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_01.save(\"Lenders_Club_mdl-01_2HL_2109061245.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb19b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add741d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e45d60",
   "metadata": {},
   "source": [
    "### b) Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca11892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential\n",
    "nn_model_02 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model_02.add(Dense(units = 19, activation = \"relu\"))\n",
    "nn_model_02.add(Dropout(rate=0.3))\n",
    "\n",
    "# Hidden layer\n",
    "nn_model_02.add(Dense(units = 10, activation = \"relu\"))\n",
    "nn_model_02.add(Dropout(rate=0.3))\n",
    "\n",
    "# Output layer with sigmoid activation\n",
    "nn_model_02.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model:\n",
    "nn_model_02.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387280f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5612e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c49c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972f9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aebf992d",
   "metadata": {},
   "source": [
    "## 3) Evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefe66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898700e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e60f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8c01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0cc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7807950a",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc24a21",
   "metadata": {},
   "source": [
    "### More on data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
