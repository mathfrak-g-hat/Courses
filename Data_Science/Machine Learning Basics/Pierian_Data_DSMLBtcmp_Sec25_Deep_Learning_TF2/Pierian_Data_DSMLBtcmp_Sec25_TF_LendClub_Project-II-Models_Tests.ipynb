{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeec06",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 Project - Predicting Loan Repayment\n",
    "# Part II-Bis: Test Models\n",
    "### A. J. Zerouali 2021/09/09\n",
    "\n",
    "* This is the project in Section 25 of Pierian Data's DSML course. It's covered (with solutions) in Lectures 143-151.\n",
    "* I'm doing trials and errors on the model to predict defaults on loans.\n",
    "* In the previous 4 attemps, the neural nets were about 82% accurate on predicting that a loan was going to be repaid, and about 51% accurate on predicting defaults.\n",
    "* In this notebook I will: (1) Shuffle the data to see if it's a data issue; (2) Remove some features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3297846",
   "metadata": {},
   "source": [
    "## 1) Preliminaries\n",
    "\n",
    "### a) Imports\n",
    "\n",
    "* First the obvious imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af4f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b0f16",
   "metadata": {},
   "source": [
    "* Here I'll load the clean data file that I made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lnclb_data = pd.read_csv(\"lenders_club_data_clean_AJZer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31993db2",
   "metadata": {},
   "source": [
    "* Here I'll load the second clean data file that I made, containing 35 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb60a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lnclb_data = pd.read_csv(\"lenders_club_data_clean_35feat_210909_AJZer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3240e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'loan_amnt', 'term', 'int_rate', 'installment',\n",
       "       'annual_inc', 'dti', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util',\n",
       "       'total_acc', 'pub_rec_bankruptcies', 'mort_acc',\n",
       "       'verification_status_Source Verified', 'verification_status_Verified',\n",
       "       'application_type_INDIVIDUAL', 'application_type_JOINT',\n",
       "       'initial_list_status_w', 'purpose_credit_card',\n",
       "       'purpose_debt_consolidation', 'purpose_educational',\n",
       "       'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase',\n",
       "       'purpose_medical', 'purpose_moving', 'purpose_other',\n",
       "       'purpose_renewable_energy', 'purpose_small_business',\n",
       "       'purpose_vacation', 'purpose_wedding', 'OTHER', 'OWN', 'RENT',\n",
       "       'loan_status_int'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71aeaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column\n",
    "df_lnclb_data = df_lnclb_data.drop(labels = 'Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d946b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lnclb_info = pd.read_csv(\"lending_club_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588d7c3",
   "metadata": {},
   "source": [
    "* The second csv gives a description of the columns of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7e7438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>...</th>\n",
       "      <th>purpose_moving</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_renewable_energy</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_vacation</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>OWN</th>\n",
       "      <th>RENT</th>\n",
       "      <th>loan_status_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.44</td>\n",
       "      <td>329.48</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>26.24</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36369.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99</td>\n",
       "      <td>265.68</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>22.05</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20131.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.49</td>\n",
       "      <td>506.97</td>\n",
       "      <td>43057.0</td>\n",
       "      <td>12.79</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6.49</td>\n",
       "      <td>220.65</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>60</td>\n",
       "      <td>17.27</td>\n",
       "      <td>609.33</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>33.95</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24584.0</td>\n",
       "      <td>69.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395214</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>10.99</td>\n",
       "      <td>217.38</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>15.63</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>34.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395215</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>12.29</td>\n",
       "      <td>700.42</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>21.45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43263.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395216</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>9.99</td>\n",
       "      <td>161.32</td>\n",
       "      <td>56500.0</td>\n",
       "      <td>17.56</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32704.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395217</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>15.31</td>\n",
       "      <td>503.02</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>15.88</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15704.0</td>\n",
       "      <td>53.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395218</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.61</td>\n",
       "      <td>67.98</td>\n",
       "      <td>42996.0</td>\n",
       "      <td>8.32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4292.0</td>\n",
       "      <td>91.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395219 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt  term  int_rate  installment  annual_inc    dti  open_acc  \\\n",
       "0         10000.0    36     11.44       329.48    117000.0  26.24      16.0   \n",
       "1          8000.0    36     11.99       265.68     65000.0  22.05      17.0   \n",
       "2         15600.0    36     10.49       506.97     43057.0  12.79      13.0   \n",
       "3          7200.0    36      6.49       220.65     54000.0   2.60       6.0   \n",
       "4         24375.0    60     17.27       609.33     55000.0  33.95      13.0   \n",
       "...           ...   ...       ...          ...         ...    ...       ...   \n",
       "395214    10000.0    60     10.99       217.38     40000.0  15.63       6.0   \n",
       "395215    21000.0    36     12.29       700.42    110000.0  21.45       6.0   \n",
       "395216     5000.0    36      9.99       161.32     56500.0  17.56      15.0   \n",
       "395217    21000.0    60     15.31       503.02     64000.0  15.88       9.0   \n",
       "395218     2000.0    36     13.61        67.98     42996.0   8.32       3.0   \n",
       "\n",
       "        pub_rec  revol_bal  revol_util  ...  purpose_moving  purpose_other  \\\n",
       "0           0.0    36369.0        41.8  ...               0              0   \n",
       "1           0.0    20131.0        53.3  ...               0              0   \n",
       "2           0.0    11987.0        92.2  ...               0              0   \n",
       "3           0.0     5472.0        21.5  ...               0              0   \n",
       "4           0.0    24584.0        69.8  ...               0              0   \n",
       "...         ...        ...         ...  ...             ...            ...   \n",
       "395214      0.0     1990.0        34.3  ...               0              0   \n",
       "395215      0.0    43263.0        95.7  ...               0              0   \n",
       "395216      0.0    32704.0        66.9  ...               0              0   \n",
       "395217      0.0    15704.0        53.8  ...               0              0   \n",
       "395218      0.0     4292.0        91.3  ...               0              0   \n",
       "\n",
       "        purpose_renewable_energy  purpose_small_business  purpose_vacation  \\\n",
       "0                              0                       0                 1   \n",
       "1                              0                       0                 0   \n",
       "2                              0                       0                 0   \n",
       "3                              0                       0                 0   \n",
       "4                              0                       0                 0   \n",
       "...                          ...                     ...               ...   \n",
       "395214                         0                       0                 0   \n",
       "395215                         0                       0                 0   \n",
       "395216                         0                       0                 0   \n",
       "395217                         0                       0                 0   \n",
       "395218                         0                       0                 0   \n",
       "\n",
       "        purpose_wedding  OTHER  OWN  RENT  loan_status_int  \n",
       "0                     0      0    0     1                1  \n",
       "1                     0      0    0     0                1  \n",
       "2                     0      0    0     1                1  \n",
       "3                     0      0    0     1                1  \n",
       "4                     0      0    0     0                0  \n",
       "...                 ...    ...  ...   ...              ...  \n",
       "395214                0      0    0     1                1  \n",
       "395215                0      0    0     0                1  \n",
       "395216                0      0    0     1                1  \n",
       "395217                0      0    0     0                1  \n",
       "395218                0      0    0     1                1  \n",
       "\n",
       "[395219 rows x 35 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2cce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 395219 entries, 0 to 395218\n",
      "Data columns (total 35 columns):\n",
      " #   Column                               Non-Null Count   Dtype  \n",
      "---  ------                               --------------   -----  \n",
      " 0   loan_amnt                            395219 non-null  float64\n",
      " 1   term                                 395219 non-null  int64  \n",
      " 2   int_rate                             395219 non-null  float64\n",
      " 3   installment                          395219 non-null  float64\n",
      " 4   annual_inc                           395219 non-null  float64\n",
      " 5   dti                                  395219 non-null  float64\n",
      " 6   open_acc                             395219 non-null  float64\n",
      " 7   pub_rec                              395219 non-null  float64\n",
      " 8   revol_bal                            395219 non-null  float64\n",
      " 9   revol_util                           395219 non-null  float64\n",
      " 10  total_acc                            395219 non-null  float64\n",
      " 11  pub_rec_bankruptcies                 395219 non-null  float64\n",
      " 12  mort_acc                             395219 non-null  float64\n",
      " 13  verification_status_Source Verified  395219 non-null  int64  \n",
      " 14  verification_status_Verified         395219 non-null  int64  \n",
      " 15  application_type_INDIVIDUAL          395219 non-null  int64  \n",
      " 16  application_type_JOINT               395219 non-null  int64  \n",
      " 17  initial_list_status_w                395219 non-null  int64  \n",
      " 18  purpose_credit_card                  395219 non-null  int64  \n",
      " 19  purpose_debt_consolidation           395219 non-null  int64  \n",
      " 20  purpose_educational                  395219 non-null  int64  \n",
      " 21  purpose_home_improvement             395219 non-null  int64  \n",
      " 22  purpose_house                        395219 non-null  int64  \n",
      " 23  purpose_major_purchase               395219 non-null  int64  \n",
      " 24  purpose_medical                      395219 non-null  int64  \n",
      " 25  purpose_moving                       395219 non-null  int64  \n",
      " 26  purpose_other                        395219 non-null  int64  \n",
      " 27  purpose_renewable_energy             395219 non-null  int64  \n",
      " 28  purpose_small_business               395219 non-null  int64  \n",
      " 29  purpose_vacation                     395219 non-null  int64  \n",
      " 30  purpose_wedding                      395219 non-null  int64  \n",
      " 31  OTHER                                395219 non-null  int64  \n",
      " 32  OWN                                  395219 non-null  int64  \n",
      " 33  RENT                                 395219 non-null  int64  \n",
      " 34  loan_status_int                      395219 non-null  int64  \n",
      "dtypes: float64(12), int64(23)\n",
      "memory usage: 105.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_lnclb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6224313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    317696\n",
       "0     77523\n",
       "Name: loan_status_int, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data[\"loan_status_int\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce73937",
   "metadata": {},
   "source": [
    "* Next, import the needed data structures and functions from TensorFlow and Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93ab985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sklearn ####\n",
    "# train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Min-max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a30dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tensorflow.keras ####\n",
    "# Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Dense and dopout layers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Callbacks: EarlyStopping and Tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a91a5",
   "metadata": {},
   "source": [
    "* Let's extract random samples for training/validation and for testing. I didn't find a Pandas method that directly shuffles the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b985b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val = df_lnclb_data.sample(n=360000, axis =0, random_state =101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd6a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_lnclb_data.sample(n=36030, axis =0, random_state =102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd3ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_lnclb_data.sample(n=35219, axis =0, random_state =102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eadea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35219 entries, 272466 to 162034\n",
      "Data columns (total 35 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   loan_amnt                            35219 non-null  float64\n",
      " 1   term                                 35219 non-null  int64  \n",
      " 2   int_rate                             35219 non-null  float64\n",
      " 3   installment                          35219 non-null  float64\n",
      " 4   annual_inc                           35219 non-null  float64\n",
      " 5   dti                                  35219 non-null  float64\n",
      " 6   open_acc                             35219 non-null  float64\n",
      " 7   pub_rec                              35219 non-null  float64\n",
      " 8   revol_bal                            35219 non-null  float64\n",
      " 9   revol_util                           35219 non-null  float64\n",
      " 10  total_acc                            35219 non-null  float64\n",
      " 11  pub_rec_bankruptcies                 35219 non-null  float64\n",
      " 12  mort_acc                             35219 non-null  float64\n",
      " 13  verification_status_Source Verified  35219 non-null  int64  \n",
      " 14  verification_status_Verified         35219 non-null  int64  \n",
      " 15  application_type_INDIVIDUAL          35219 non-null  int64  \n",
      " 16  application_type_JOINT               35219 non-null  int64  \n",
      " 17  initial_list_status_w                35219 non-null  int64  \n",
      " 18  purpose_credit_card                  35219 non-null  int64  \n",
      " 19  purpose_debt_consolidation           35219 non-null  int64  \n",
      " 20  purpose_educational                  35219 non-null  int64  \n",
      " 21  purpose_home_improvement             35219 non-null  int64  \n",
      " 22  purpose_house                        35219 non-null  int64  \n",
      " 23  purpose_major_purchase               35219 non-null  int64  \n",
      " 24  purpose_medical                      35219 non-null  int64  \n",
      " 25  purpose_moving                       35219 non-null  int64  \n",
      " 26  purpose_other                        35219 non-null  int64  \n",
      " 27  purpose_renewable_energy             35219 non-null  int64  \n",
      " 28  purpose_small_business               35219 non-null  int64  \n",
      " 29  purpose_vacation                     35219 non-null  int64  \n",
      " 30  purpose_wedding                      35219 non-null  int64  \n",
      " 31  OTHER                                35219 non-null  int64  \n",
      " 32  OWN                                  35219 non-null  int64  \n",
      " 33  RENT                                 35219 non-null  int64  \n",
      " 34  loan_status_int                      35219 non-null  int64  \n",
      "dtypes: float64(12), int64(23)\n",
      "memory usage: 9.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9deb025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sample in module pandas.core.generic:\n",
      "\n",
      "sample(self: 'FrameOrSeries', n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) -> 'FrameOrSeries'\n",
      "    Return a random sample of items from an axis of object.\n",
      "    \n",
      "    You can use `random_state` for reproducibility.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int, optional\n",
      "        Number of items from axis to return. Cannot be used with `frac`.\n",
      "        Default = 1 if `frac` = None.\n",
      "    frac : float, optional\n",
      "        Fraction of axis items to return. Cannot be used with `n`.\n",
      "    replace : bool, default False\n",
      "        Allow or disallow sampling of the same row more than once.\n",
      "    weights : str or ndarray-like, optional\n",
      "        Default 'None' results in equal probability weighting.\n",
      "        If passed a Series, will align with target object on index. Index\n",
      "        values in weights not found in sampled object will be ignored and\n",
      "        index values in sampled object not in weights will be assigned\n",
      "        weights of zero.\n",
      "        If called on a DataFrame, will accept the name of a column\n",
      "        when axis = 0.\n",
      "        Unless weights are a Series, weights must be same length as axis\n",
      "        being sampled.\n",
      "        If weights do not sum to 1, they will be normalized to sum to 1.\n",
      "        Missing values in the weights column will be treated as zero.\n",
      "        Infinite values not allowed.\n",
      "    random_state : int, array-like, BitGenerator, np.random.RandomState, optional\n",
      "        If int, array-like, or BitGenerator (NumPy>=1.17), seed for\n",
      "        random number generator\n",
      "        If np.random.RandomState, use as numpy RandomState object.\n",
      "    \n",
      "        .. versionchanged:: 1.1.0\n",
      "    \n",
      "            array-like and BitGenerator (for NumPy>=1.17) object now passed to\n",
      "            np.random.RandomState() as seed\n",
      "    \n",
      "    axis : {0 or ‘index’, 1 or ‘columns’, None}, default None\n",
      "        Axis to sample. Accepts axis number or name. Default is stat axis\n",
      "        for given data type (0 for Series and DataFrames).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series or DataFrame\n",
      "        A new object of same type as caller containing `n` items randomly\n",
      "        sampled from the caller object.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrameGroupBy.sample: Generates random samples from each group of a\n",
      "        DataFrame object.\n",
      "    SeriesGroupBy.sample: Generates random samples from each group of a\n",
      "        Series object.\n",
      "    numpy.random.choice: Generates a random sample from a given 1-D numpy\n",
      "        array.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    If `frac` > 1, `replacement` should be set to `True`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
      "    ...                    'num_wings': [2, 0, 0, 0],\n",
      "    ...                    'num_specimen_seen': [10, 2, 1, 8]},\n",
      "    ...                   index=['falcon', 'dog', 'spider', 'fish'])\n",
      "    >>> df\n",
      "            num_legs  num_wings  num_specimen_seen\n",
      "    falcon         2          2                 10\n",
      "    dog            4          0                  2\n",
      "    spider         8          0                  1\n",
      "    fish           0          0                  8\n",
      "    \n",
      "    Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n",
      "    Note that we use `random_state` to ensure the reproducibility of\n",
      "    the examples.\n",
      "    \n",
      "    >>> df['num_legs'].sample(n=3, random_state=1)\n",
      "    fish      0\n",
      "    spider    8\n",
      "    falcon    2\n",
      "    Name: num_legs, dtype: int64\n",
      "    \n",
      "    A random 50% sample of the ``DataFrame`` with replacement:\n",
      "    \n",
      "    >>> df.sample(frac=0.5, replace=True, random_state=1)\n",
      "          num_legs  num_wings  num_specimen_seen\n",
      "    dog          4          0                  2\n",
      "    fish         0          0                  8\n",
      "    \n",
      "    An upsample sample of the ``DataFrame`` with replacement:\n",
      "    Note that `replace` parameter has to be `True` for `frac` parameter > 1.\n",
      "    \n",
      "    >>> df.sample(frac=2, replace=True, random_state=1)\n",
      "            num_legs  num_wings  num_specimen_seen\n",
      "    dog            4          0                  2\n",
      "    fish           0          0                  8\n",
      "    falcon         2          2                 10\n",
      "    falcon         2          2                 10\n",
      "    fish           0          0                  8\n",
      "    dog            4          0                  2\n",
      "    fish           0          0                  8\n",
      "    dog            4          0                  2\n",
      "    \n",
      "    Using a DataFrame column as weights. Rows with larger value in the\n",
      "    `num_specimen_seen` column are more likely to be sampled.\n",
      "    \n",
      "    >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n",
      "            num_legs  num_wings  num_specimen_seen\n",
      "    falcon         2          2                 10\n",
      "    fish           0          0                  8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acc550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db571268",
   "metadata": {},
   "source": [
    "## Model 05\n",
    "\n",
    "* For this first model, we'll use all 19 features. Will set dropouts at 0.3.\n",
    "* We'll use training batches of size $2^{14}=16384$.\n",
    "* We'll optimize with ADAM and use the cross-entropy loss\n",
    "* I will try with 3 dense hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e2f94",
   "metadata": {},
   "source": [
    "### a) Data preprocessing\n",
    "\n",
    "* We start with the full set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "X = df_train_val.drop(labels=\"loan_status_int\", axis =1).values\n",
    "y = df_train_val[\"loan_status_int\"].values\n",
    "\n",
    "# Test data\n",
    "X_Test = df_test.drop(labels=\"loan_status_int\", axis =1).values\n",
    "y_Test = df_test[\"loan_status_int\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fa1e3",
   "metadata": {},
   "source": [
    "* Apply train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de38e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, X_val, y_train, y_val] = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42918d",
   "metadata": {},
   "source": [
    "* Create the MinMaxScaler and fit to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2caeeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = data_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0653de",
   "metadata": {},
   "source": [
    "* Scale the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2a2b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data_scaler.transform(X_val)\n",
    "X_Test = data_scaler.transform(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8cd9fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241200, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c79e9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118800, 19)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9a0cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36030, 19)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7b6ee",
   "metadata": {},
   "source": [
    "### b) Model construction and callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ce7b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential\n",
    "nn_model_05 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model_05.add(Dense(units = 19, activation = \"relu\"))\n",
    "\n",
    "# Hidden layers\n",
    "nn_model_05.add(Dense(units = 40, activation = \"relu\"))\n",
    "\n",
    "nn_model_05.add(Dense(units = 20, activation = \"relu\"))\n",
    "\n",
    "nn_model_05.add(Dense(units = 10, activation = \"relu\"))\n",
    "\n",
    "# Output layer with sigmoid activation\n",
    "nn_model_05.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model:\n",
    "nn_model_05.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb1bce",
   "metadata": {},
   "source": [
    "* Create EarlyStop. I've changed the patience from 50 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "060604fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_05 = EarlyStopping(monitor='val_loss', patience=25, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e845b0",
   "metadata": {},
   "source": [
    "### c) Training\n",
    "\n",
    "* I'll skip the board for this  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfbf34b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "15/15 [==============================] - 1s 32ms/step - loss: 0.5977 - val_loss: 0.5483\n",
      "Epoch 2/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.5191 - val_loss: 0.5028\n",
      "Epoch 3/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4998 - val_loss: 0.4944\n",
      "Epoch 4/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4900 - val_loss: 0.4860\n",
      "Epoch 5/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4830 - val_loss: 0.4796\n",
      "Epoch 6/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4777 - val_loss: 0.4746\n",
      "Epoch 7/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4736 - val_loss: 0.4705\n",
      "Epoch 8/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4702 - val_loss: 0.4673\n",
      "Epoch 9/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4675 - val_loss: 0.4646\n",
      "Epoch 10/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4650 - val_loss: 0.4622\n",
      "Epoch 11/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4628 - val_loss: 0.4604\n",
      "Epoch 12/1500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4612 - val_loss: 0.4591\n",
      "Epoch 13/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4601 - val_loss: 0.4582\n",
      "Epoch 14/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4594 - val_loss: 0.4576\n",
      "Epoch 15/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4589 - val_loss: 0.4572\n",
      "Epoch 16/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4585 - val_loss: 0.4569\n",
      "Epoch 17/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4582 - val_loss: 0.4566\n",
      "Epoch 18/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4579 - val_loss: 0.4564\n",
      "Epoch 19/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4577 - val_loss: 0.4562\n",
      "Epoch 20/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4575 - val_loss: 0.4560\n",
      "Epoch 21/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4574 - val_loss: 0.4559\n",
      "Epoch 22/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4573 - val_loss: 0.4558\n",
      "Epoch 23/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4571 - val_loss: 0.4558\n",
      "Epoch 24/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4570 - val_loss: 0.4557\n",
      "Epoch 25/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4569 - val_loss: 0.4558\n",
      "Epoch 26/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4568 - val_loss: 0.4555\n",
      "Epoch 27/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4567 - val_loss: 0.4554\n",
      "Epoch 28/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4566 - val_loss: 0.4555\n",
      "Epoch 29/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4565 - val_loss: 0.4553\n",
      "Epoch 30/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4564 - val_loss: 0.4553\n",
      "Epoch 31/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4564 - val_loss: 0.4553\n",
      "Epoch 32/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4564 - val_loss: 0.4552\n",
      "Epoch 33/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4563 - val_loss: 0.4552\n",
      "Epoch 34/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4563 - val_loss: 0.4551\n",
      "Epoch 35/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4562 - val_loss: 0.4551\n",
      "Epoch 36/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4561 - val_loss: 0.4551\n",
      "Epoch 37/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4561 - val_loss: 0.4550\n",
      "Epoch 38/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4561 - val_loss: 0.4550\n",
      "Epoch 39/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4560 - val_loss: 0.4549\n",
      "Epoch 40/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4559 - val_loss: 0.4549\n",
      "Epoch 41/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4559 - val_loss: 0.4549\n",
      "Epoch 42/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4559 - val_loss: 0.4549\n",
      "Epoch 43/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4558 - val_loss: 0.4549\n",
      "Epoch 44/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4558 - val_loss: 0.4549\n",
      "Epoch 45/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4557 - val_loss: 0.4548\n",
      "Epoch 46/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4557 - val_loss: 0.4547\n",
      "Epoch 47/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4557 - val_loss: 0.4548\n",
      "Epoch 48/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4556 - val_loss: 0.4546\n",
      "Epoch 49/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4556 - val_loss: 0.4547\n",
      "Epoch 50/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4556 - val_loss: 0.4547\n",
      "Epoch 51/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4556 - val_loss: 0.4546\n",
      "Epoch 52/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4555 - val_loss: 0.4546\n",
      "Epoch 53/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4556 - val_loss: 0.4547\n",
      "Epoch 54/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4554 - val_loss: 0.4546\n",
      "Epoch 55/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4554 - val_loss: 0.4545\n",
      "Epoch 56/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4553 - val_loss: 0.4546\n",
      "Epoch 57/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4554 - val_loss: 0.4543\n",
      "Epoch 58/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4553 - val_loss: 0.4544\n",
      "Epoch 59/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4553 - val_loss: 0.4543\n",
      "Epoch 60/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4552 - val_loss: 0.4545\n",
      "Epoch 61/1500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4552 - val_loss: 0.4543\n",
      "Epoch 62/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4551 - val_loss: 0.4543\n",
      "Epoch 63/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4551 - val_loss: 0.4542\n",
      "Epoch 64/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4550 - val_loss: 0.4542\n",
      "Epoch 65/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4551 - val_loss: 0.4542\n",
      "Epoch 66/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4549 - val_loss: 0.4541\n",
      "Epoch 67/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4549 - val_loss: 0.4541\n",
      "Epoch 68/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4548 - val_loss: 0.4541\n",
      "Epoch 69/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4548 - val_loss: 0.4540\n",
      "Epoch 70/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4548 - val_loss: 0.4540\n",
      "Epoch 71/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4548 - val_loss: 0.4540\n",
      "Epoch 72/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4547 - val_loss: 0.4540\n",
      "Epoch 73/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4547 - val_loss: 0.4539\n",
      "Epoch 74/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4547 - val_loss: 0.4539\n",
      "Epoch 75/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4549 - val_loss: 0.4540\n",
      "Epoch 76/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4547 - val_loss: 0.4539\n",
      "Epoch 77/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4546 - val_loss: 0.4538\n",
      "Epoch 78/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4546 - val_loss: 0.4539\n",
      "Epoch 79/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4546 - val_loss: 0.4538\n",
      "Epoch 80/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4546 - val_loss: 0.4539\n",
      "Epoch 81/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4544 - val_loss: 0.4538\n",
      "Epoch 82/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4547 - val_loss: 0.4537\n",
      "Epoch 83/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4544 - val_loss: 0.4538\n",
      "Epoch 84/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4544 - val_loss: 0.4537\n",
      "Epoch 85/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4543 - val_loss: 0.4536\n",
      "Epoch 86/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4543 - val_loss: 0.4536\n",
      "Epoch 87/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4543 - val_loss: 0.4536\n",
      "Epoch 88/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4542 - val_loss: 0.4536\n",
      "Epoch 89/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4542 - val_loss: 0.4535\n",
      "Epoch 90/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4541 - val_loss: 0.4536\n",
      "Epoch 91/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4542 - val_loss: 0.4535\n",
      "Epoch 92/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4541 - val_loss: 0.4535\n",
      "Epoch 93/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4541 - val_loss: 0.4535\n",
      "Epoch 94/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4541 - val_loss: 0.4540\n",
      "Epoch 95/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4542 - val_loss: 0.4539\n",
      "Epoch 96/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4541 - val_loss: 0.4534\n",
      "Epoch 97/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4540 - val_loss: 0.4534\n",
      "Epoch 98/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4539 - val_loss: 0.4534\n",
      "Epoch 99/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4538 - val_loss: 0.4533\n",
      "Epoch 100/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4538 - val_loss: 0.4533\n",
      "Epoch 101/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4538 - val_loss: 0.4533\n",
      "Epoch 102/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4538 - val_loss: 0.4532\n",
      "Epoch 103/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4537 - val_loss: 0.4534\n",
      "Epoch 104/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4538 - val_loss: 0.4534\n",
      "Epoch 105/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4539 - val_loss: 0.4534\n",
      "Epoch 106/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4538 - val_loss: 0.4534\n",
      "Epoch 107/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4537 - val_loss: 0.4532\n",
      "Epoch 108/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4535 - val_loss: 0.4532\n",
      "Epoch 109/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4535 - val_loss: 0.4531\n",
      "Epoch 110/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4535 - val_loss: 0.4531\n",
      "Epoch 111/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4535 - val_loss: 0.4534\n",
      "Epoch 112/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4535 - val_loss: 0.4530\n",
      "Epoch 113/1500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4534 - val_loss: 0.4531\n",
      "Epoch 114/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4534 - val_loss: 0.4531\n",
      "Epoch 115/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4534 - val_loss: 0.4531\n",
      "Epoch 116/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4536 - val_loss: 0.4531\n",
      "Epoch 117/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4534 - val_loss: 0.4531\n",
      "Epoch 118/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4533 - val_loss: 0.4530\n",
      "Epoch 119/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4533 - val_loss: 0.4531\n",
      "Epoch 120/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4532 - val_loss: 0.4530\n",
      "Epoch 121/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4532 - val_loss: 0.4529\n",
      "Epoch 122/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4532 - val_loss: 0.4529\n",
      "Epoch 123/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4533 - val_loss: 0.4529\n",
      "Epoch 124/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4532 - val_loss: 0.4530\n",
      "Epoch 125/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4532 - val_loss: 0.4529\n",
      "Epoch 126/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4531 - val_loss: 0.4528\n",
      "Epoch 127/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4530 - val_loss: 0.4530\n",
      "Epoch 128/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4531 - val_loss: 0.4528\n",
      "Epoch 129/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4530 - val_loss: 0.4528\n",
      "Epoch 130/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4529 - val_loss: 0.4528\n",
      "Epoch 131/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4530 - val_loss: 0.4528\n",
      "Epoch 132/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4531 - val_loss: 0.4527\n",
      "Epoch 133/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4529 - val_loss: 0.4528\n",
      "Epoch 134/1500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4528 - val_loss: 0.4528\n",
      "Epoch 135/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4528 - val_loss: 0.4528\n",
      "Epoch 136/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4528 - val_loss: 0.4527\n",
      "Epoch 137/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4528 - val_loss: 0.4528\n",
      "Epoch 138/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4530 - val_loss: 0.4527\n",
      "Epoch 139/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4531 - val_loss: 0.4526\n",
      "Epoch 140/1500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4527 - val_loss: 0.4529\n",
      "Epoch 141/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4528 - val_loss: 0.4528\n",
      "Epoch 142/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4528 - val_loss: 0.4526\n",
      "Epoch 143/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4528 - val_loss: 0.4528\n",
      "Epoch 144/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4526 - val_loss: 0.4527\n",
      "Epoch 145/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4527 - val_loss: 0.4526\n",
      "Epoch 146/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4525 - val_loss: 0.4526\n",
      "Epoch 147/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4526 - val_loss: 0.4527\n",
      "Epoch 148/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4526 - val_loss: 0.4527\n",
      "Epoch 149/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4526 - val_loss: 0.4525\n",
      "Epoch 150/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4525 - val_loss: 0.4526\n",
      "Epoch 151/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4525 - val_loss: 0.4525\n",
      "Epoch 152/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4524 - val_loss: 0.4524\n",
      "Epoch 153/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4524 - val_loss: 0.4524\n",
      "Epoch 154/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4526 - val_loss: 0.4526\n",
      "Epoch 155/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4526 - val_loss: 0.4529\n",
      "Epoch 156/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4524 - val_loss: 0.4526\n",
      "Epoch 157/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4523 - val_loss: 0.4524\n",
      "Epoch 158/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4523 - val_loss: 0.4526\n",
      "Epoch 159/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4524 - val_loss: 0.4524\n",
      "Epoch 160/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4523 - val_loss: 0.4524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4523 - val_loss: 0.4524\n",
      "Epoch 162/1500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4523 - val_loss: 0.4523\n",
      "Epoch 163/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4521 - val_loss: 0.4523\n",
      "Epoch 164/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4522 - val_loss: 0.4524\n",
      "Epoch 165/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4521 - val_loss: 0.4527\n",
      "Epoch 166/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4522 - val_loss: 0.4522\n",
      "Epoch 167/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4522 - val_loss: 0.4523\n",
      "Epoch 168/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4522 - val_loss: 0.4522\n",
      "Epoch 169/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4520 - val_loss: 0.4523\n",
      "Epoch 170/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4520 - val_loss: 0.4521\n",
      "Epoch 171/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4520 - val_loss: 0.4522\n",
      "Epoch 172/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4519 - val_loss: 0.4522\n",
      "Epoch 173/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4520 - val_loss: 0.4523\n",
      "Epoch 174/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4520 - val_loss: 0.4523\n",
      "Epoch 175/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4521 - val_loss: 0.4523\n",
      "Epoch 176/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4521 - val_loss: 0.4523\n",
      "Epoch 177/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4519 - val_loss: 0.4524\n",
      "Epoch 178/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4521 - val_loss: 0.4521\n",
      "Epoch 179/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4519 - val_loss: 0.4521\n",
      "Epoch 180/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4518 - val_loss: 0.4521\n",
      "Epoch 181/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4519 - val_loss: 0.4520\n",
      "Epoch 182/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4520 - val_loss: 0.4520\n",
      "Epoch 183/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4518 - val_loss: 0.4524\n",
      "Epoch 184/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4517 - val_loss: 0.4519\n",
      "Epoch 185/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4517 - val_loss: 0.4520\n",
      "Epoch 186/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4517 - val_loss: 0.4520\n",
      "Epoch 187/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4518 - val_loss: 0.4519\n",
      "Epoch 188/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4518 - val_loss: 0.4519\n",
      "Epoch 189/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4518 - val_loss: 0.4520\n",
      "Epoch 190/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4516 - val_loss: 0.4518\n",
      "Epoch 191/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4516 - val_loss: 0.4519\n",
      "Epoch 192/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4516 - val_loss: 0.4519\n",
      "Epoch 193/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4515 - val_loss: 0.4518\n",
      "Epoch 194/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4515 - val_loss: 0.4518\n",
      "Epoch 195/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4516 - val_loss: 0.4519\n",
      "Epoch 196/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4514 - val_loss: 0.4518\n",
      "Epoch 197/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4519\n",
      "Epoch 198/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4514 - val_loss: 0.4519\n",
      "Epoch 199/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4515 - val_loss: 0.4517\n",
      "Epoch 200/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4514 - val_loss: 0.4518\n",
      "Epoch 201/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4518\n",
      "Epoch 202/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4516\n",
      "Epoch 203/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4516\n",
      "Epoch 204/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4516\n",
      "Epoch 205/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4512 - val_loss: 0.4515\n",
      "Epoch 206/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4515 - val_loss: 0.4522\n",
      "Epoch 207/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4513 - val_loss: 0.4515\n",
      "Epoch 208/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4514 - val_loss: 0.4516\n",
      "Epoch 209/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4513 - val_loss: 0.4515\n",
      "Epoch 210/1500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4511 - val_loss: 0.4518\n",
      "Epoch 211/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4512 - val_loss: 0.4515\n",
      "Epoch 212/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4511 - val_loss: 0.4515\n",
      "Epoch 213/1500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4510 - val_loss: 0.4516\n",
      "Epoch 214/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4512 - val_loss: 0.4521\n",
      "Epoch 215/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4513 - val_loss: 0.4519\n",
      "Epoch 216/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4512 - val_loss: 0.4516\n",
      "Epoch 217/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4511 - val_loss: 0.4514\n",
      "Epoch 218/1500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4510 - val_loss: 0.4515\n",
      "Epoch 219/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4509 - val_loss: 0.4514\n",
      "Epoch 220/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4508 - val_loss: 0.4513\n",
      "Epoch 221/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4510 - val_loss: 0.4516\n",
      "Epoch 222/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4508 - val_loss: 0.4515\n",
      "Epoch 223/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4509 - val_loss: 0.4514\n",
      "Epoch 224/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4509 - val_loss: 0.4514\n",
      "Epoch 225/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4509 - val_loss: 0.4512\n",
      "Epoch 226/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4513\n",
      "Epoch 227/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4513\n",
      "Epoch 228/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4508 - val_loss: 0.4513\n",
      "Epoch 229/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4509 - val_loss: 0.4513\n",
      "Epoch 230/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4506 - val_loss: 0.4512\n",
      "Epoch 231/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4514\n",
      "Epoch 232/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4512\n",
      "Epoch 233/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4506 - val_loss: 0.4511\n",
      "Epoch 234/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4506 - val_loss: 0.4512\n",
      "Epoch 235/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4506 - val_loss: 0.4514\n",
      "Epoch 236/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4506 - val_loss: 0.4512\n",
      "Epoch 237/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4512\n",
      "Epoch 238/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4511\n",
      "Epoch 239/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4511\n",
      "Epoch 240/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4504 - val_loss: 0.4514\n",
      "Epoch 241/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4511\n",
      "Epoch 242/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4512\n",
      "Epoch 243/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4504 - val_loss: 0.4512\n",
      "Epoch 244/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4504 - val_loss: 0.4512\n",
      "Epoch 245/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4514\n",
      "Epoch 246/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4514\n",
      "Epoch 247/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4505 - val_loss: 0.4511\n",
      "Epoch 248/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4517\n",
      "Epoch 249/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4507 - val_loss: 0.4510\n",
      "Epoch 250/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4504 - val_loss: 0.4517\n",
      "Epoch 251/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4503 - val_loss: 0.4510\n",
      "Epoch 252/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4503 - val_loss: 0.4511\n",
      "Epoch 253/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4504 - val_loss: 0.4510\n",
      "Epoch 254/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4506 - val_loss: 0.4516\n",
      "Epoch 255/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4502 - val_loss: 0.4510\n",
      "Epoch 256/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4503 - val_loss: 0.4511\n",
      "Epoch 257/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4502 - val_loss: 0.4510\n",
      "Epoch 258/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4501 - val_loss: 0.4510\n",
      "Epoch 259/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4501 - val_loss: 0.4510\n",
      "Epoch 260/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4502 - val_loss: 0.4510\n",
      "Epoch 261/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4501 - val_loss: 0.4510\n",
      "Epoch 262/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4500 - val_loss: 0.4508\n",
      "Epoch 263/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4500 - val_loss: 0.4509\n",
      "Epoch 264/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4500 - val_loss: 0.4512\n",
      "Epoch 265/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4500 - val_loss: 0.4508\n",
      "Epoch 266/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4502 - val_loss: 0.4513\n",
      "Epoch 267/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4500 - val_loss: 0.4508\n",
      "Epoch 268/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4499 - val_loss: 0.4509\n",
      "Epoch 269/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4499 - val_loss: 0.4507\n",
      "Epoch 270/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4501 - val_loss: 0.4508\n",
      "Epoch 271/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4502 - val_loss: 0.4518\n",
      "Epoch 272/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4501 - val_loss: 0.4508\n",
      "Epoch 273/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4509\n",
      "Epoch 274/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4499 - val_loss: 0.4509\n",
      "Epoch 275/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4499 - val_loss: 0.4508\n",
      "Epoch 276/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4507\n",
      "Epoch 277/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4497 - val_loss: 0.4507\n",
      "Epoch 278/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4509\n",
      "Epoch 279/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4497 - val_loss: 0.4507\n",
      "Epoch 280/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4497 - val_loss: 0.4508\n",
      "Epoch 281/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4507\n",
      "Epoch 282/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4507\n",
      "Epoch 283/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4508\n",
      "Epoch 284/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4508\n",
      "Epoch 285/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4507\n",
      "Epoch 286/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4497 - val_loss: 0.4511\n",
      "Epoch 287/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4499 - val_loss: 0.4506\n",
      "Epoch 288/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4496 - val_loss: 0.4507\n",
      "Epoch 289/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4507\n",
      "Epoch 290/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4510\n",
      "Epoch 291/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4508\n",
      "Epoch 292/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4505\n",
      "Epoch 293/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4506\n",
      "Epoch 294/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4507\n",
      "Epoch 295/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4498 - val_loss: 0.4507\n",
      "Epoch 296/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4506\n",
      "Epoch 297/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4506\n",
      "Epoch 298/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4513\n",
      "Epoch 299/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4496 - val_loss: 0.4508\n",
      "Epoch 300/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4508\n",
      "Epoch 301/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4507\n",
      "Epoch 302/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4508\n",
      "Epoch 303/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4497 - val_loss: 0.4508\n",
      "Epoch 304/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4494 - val_loss: 0.4506\n",
      "Epoch 305/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 306/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 307/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4496 - val_loss: 0.4505\n",
      "Epoch 308/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4494 - val_loss: 0.4506\n",
      "Epoch 309/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 310/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.4506\n",
      "Epoch 311/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4494 - val_loss: 0.4505\n",
      "Epoch 312/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4505\n",
      "Epoch 313/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4506\n",
      "Epoch 314/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4494 - val_loss: 0.4505\n",
      "Epoch 315/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4506\n",
      "Epoch 316/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4506\n",
      "Epoch 317/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4493 - val_loss: 0.4513\n",
      "Epoch 318/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4494 - val_loss: 0.4504\n",
      "Epoch 319/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4504\n",
      "Epoch 320/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4506\n",
      "Epoch 321/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4491 - val_loss: 0.4506\n",
      "Epoch 322/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4491 - val_loss: 0.4504\n",
      "Epoch 323/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4512\n",
      "Epoch 324/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4505\n",
      "Epoch 325/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4505\n",
      "Epoch 326/1500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4492 - val_loss: 0.4505\n",
      "Epoch 327/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 328/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4491 - val_loss: 0.4506\n",
      "Epoch 329/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4504\n",
      "Epoch 330/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4506\n",
      "Epoch 331/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4505\n",
      "Epoch 332/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4490 - val_loss: 0.4508\n",
      "Epoch 333/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4506\n",
      "Epoch 334/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4504\n",
      "Epoch 335/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4489 - val_loss: 0.4504\n",
      "Epoch 336/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4489 - val_loss: 0.4506\n",
      "Epoch 337/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4489 - val_loss: 0.4505\n",
      "Epoch 338/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4508\n",
      "Epoch 339/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4508\n",
      "Epoch 340/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4507\n",
      "Epoch 341/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4492 - val_loss: 0.4512\n",
      "Epoch 342/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4494 - val_loss: 0.4508\n",
      "Epoch 343/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 344/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4504\n",
      "Epoch 345/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4491 - val_loss: 0.4508\n",
      "Epoch 346/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4493 - val_loss: 0.4507\n",
      "Epoch 347/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4505\n",
      "Epoch 348/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4506\n",
      "Epoch 349/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4507\n",
      "Epoch 350/1500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4490 - val_loss: 0.4504\n",
      "Epoch 351/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4488 - val_loss: 0.4506\n",
      "Epoch 352/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4487 - val_loss: 0.4507\n",
      "Epoch 353/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4490 - val_loss: 0.4505\n",
      "Epoch 354/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4489 - val_loss: 0.4504\n",
      "Epoch 355/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4488 - val_loss: 0.4505\n",
      "Epoch 356/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4489 - val_loss: 0.4505\n",
      "Epoch 357/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4488 - val_loss: 0.4507\n",
      "Epoch 358/1500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.4488 - val_loss: 0.4504\n",
      "Epoch 359/1500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4487 - val_loss: 0.4506\n",
      "Epoch 00359: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e07e22ceb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_05.fit(x = X_train, y=y_train, \\\n",
    "                validation_data=(X_val, y_val),\\\n",
    "                batch_size = 16384, validation_batch_size = 16384,\n",
    "                epochs = 1500, \\\n",
    "                callbacks=[early_stop_05],\\\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd305c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(nn_model_05.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09b736",
   "metadata": {},
   "source": [
    "### d) Prediction and evaluation\n",
    "\n",
    "* Let's look at the predictions for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53cf0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_05 = (nn_model_05.predict(X_Test) > 0.55).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905204b2",
   "metadata": {},
   "source": [
    "* Let's look at the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59a55cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.10      0.17      6990\n",
      "           1       0.82      0.98      0.89     29040\n",
      "\n",
      "    accuracy                           0.81     36030\n",
      "   macro avg       0.69      0.54      0.53     36030\n",
      "weighted avg       0.77      0.81      0.75     36030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_Test, y_pred=z_pred_05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "621f8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  699  6291]\n",
      " [  569 28471]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true = y_Test, y_pred=z_pred_05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa74ee7",
   "metadata": {},
   "source": [
    "* **For reference:** Here are the number of ones in each y vector for the **first attempt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "451990a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193979"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1570907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95495"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f11e93ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_Test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28199a",
   "metadata": {},
   "source": [
    "* Here are the number of ones in the \"new\" y vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "161e16c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193908"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3906e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95530"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "148a835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29040"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_Test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866c92f",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "Shuffling the data doesn't seem to impact the overall accuracy. There might be too many irrelevant parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde3dae6",
   "metadata": {},
   "source": [
    "## Model 06\n",
    "\n",
    "* We'll drop some features and use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13337f41",
   "metadata": {},
   "source": [
    "### a) Data preprocessing\n",
    "\n",
    "* We start by reviewing our features. In particular, we look at correlations with loan_status_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87e238e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
       "       'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'mort_acc',\n",
       "       'pub_rec_bankruptcies', 'loan_status_int', 'term_int',\n",
       "       'home_ownership_int', 'verification_status_int', 'purpose_int',\n",
       "       'initial_list_status_int', 'application_type_int', 'emp_length_int'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de20300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                 -0.059836\n",
       "int_rate                  -0.247758\n",
       "installment               -0.041082\n",
       "annual_inc                 0.053432\n",
       "dti                       -0.062413\n",
       "open_acc                  -0.028012\n",
       "pub_rec                   -0.019933\n",
       "revol_bal                  0.010892\n",
       "revol_util                -0.082341\n",
       "total_acc                  0.017893\n",
       "mort_acc                   0.068661\n",
       "pub_rec_bankruptcies      -0.009416\n",
       "loan_status_int            1.000000\n",
       "term_int                  -0.173246\n",
       "home_ownership_int         0.044559\n",
       "verification_status_int   -0.077909\n",
       "purpose_int                0.008974\n",
       "initial_list_status_int   -0.009489\n",
       "application_type_int      -0.006315\n",
       "emp_length_int             0.013154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lnclb_data.corrwith(other=df_lnclb_data[\"loan_status_int\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad267c",
   "metadata": {},
   "source": [
    "* Following these numbers, let's drop the following:\n",
    "            revol_bal                  0.010892\n",
    "            total_acc                  0.017893\n",
    "            pub_rec_bankruptcies      -0.009416\n",
    "            purpose_int                0.008974\n",
    "            initial_list_status_int   -0.009489\n",
    "            application_type_int      -0.006315\n",
    "            emp_length_int             0.013154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2f8a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val = df_lnclb_data.drop(labels= [\"revol_bal\", \"total_acc\", \"pub_rec_bankruptcies\", \"purpose_int\", \\\n",
    "                                           \"initial_list_status_int\", \"application_type_int\",\"emp_length_int\"],\\\n",
    "                                     axis =1 ).sample(n=360000, axis =0, random_state =101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e94f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_lnclb_data.drop(labels= [\"revol_bal\", \"total_acc\", \"pub_rec_bankruptcies\", \"purpose_int\", \\\n",
    "                                           \"initial_list_status_int\", \"application_type_int\",\"emp_length_int\"],\\\n",
    "                            axis =1 ).sample(n=36030, axis =0, random_state =102)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad5f5e",
   "metadata": {},
   "source": [
    "* Here's the data with reduced number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ed0f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "X = df_train_val.drop(labels=[\"loan_status_int\"], axis =1).values\n",
    "y = df_train_val[\"loan_status_int\"].values\n",
    "\n",
    "# Test data\n",
    "X_Test = df_test.drop(labels=\"loan_status_int\", axis =1).values\n",
    "y_Test = df_test[\"loan_status_int\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d28758",
   "metadata": {},
   "source": [
    "* Apply train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6974180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, X_val, y_train, y_val] = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f050b3",
   "metadata": {},
   "source": [
    "* Create the MinMaxScaler and fit to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8965757",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = data_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4b5b5",
   "metadata": {},
   "source": [
    "* Scale the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d06ef970",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data_scaler.transform(X_val)\n",
    "X_Test = data_scaler.transform(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc18c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241200, 12)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8292b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118800, 12)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b0102ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36030, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe83894",
   "metadata": {},
   "source": [
    "### b) Model construction and callback\n",
    "\n",
    "* I'll try with a network that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a8786ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential\n",
    "nn_model_06 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model_06.add(Dense(units = 12, activation = \"relu\"))\n",
    "\n",
    "# Hidden layers\n",
    "nn_model_06.add(Dense(units = 25, activation = \"relu\"))\n",
    "\n",
    "nn_model_06.add(Dense(units = 10, activation = \"relu\"))\n",
    "\n",
    "nn_model_06.add(Dense(units = 5, activation = \"relu\"))\n",
    "\n",
    "# Output layer with sigmoid activation\n",
    "nn_model_06.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model:\n",
    "nn_model_06.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46a0d9",
   "metadata": {},
   "source": [
    "* Create EarlyStop. I've changed the patience from 50 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67b37b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_06 = EarlyStopping(monitor='val_loss', patience=25, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1124bd",
   "metadata": {},
   "source": [
    "### c) Training\n",
    "\n",
    "* I'll skip the board for this  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9b9c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "15/15 [==============================] - 1s 21ms/step - loss: 0.6895 - val_loss: 0.6768\n",
      "Epoch 2/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.6628 - val_loss: 0.6455\n",
      "Epoch 3/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.6292 - val_loss: 0.6087\n",
      "Epoch 4/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.5896 - val_loss: 0.5669\n",
      "Epoch 5/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.5491 - val_loss: 0.5318\n",
      "Epoch 6/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.5233 - val_loss: 0.5169\n",
      "Epoch 7/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.5134 - val_loss: 0.5099\n",
      "Epoch 8/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.5066 - val_loss: 0.5031\n",
      "Epoch 9/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.5000 - val_loss: 0.4966\n",
      "Epoch 10/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4938 - val_loss: 0.4905\n",
      "Epoch 11/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4878 - val_loss: 0.4844\n",
      "Epoch 12/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4821 - val_loss: 0.4789\n",
      "Epoch 13/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4772 - val_loss: 0.4740\n",
      "Epoch 14/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4725 - val_loss: 0.4694\n",
      "Epoch 15/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4684 - val_loss: 0.4659\n",
      "Epoch 16/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4655 - val_loss: 0.4632\n",
      "Epoch 17/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4635 - val_loss: 0.4615\n",
      "Epoch 18/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4621 - val_loss: 0.4603\n",
      "Epoch 19/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4611 - val_loss: 0.4596\n",
      "Epoch 20/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4606 - val_loss: 0.4591\n",
      "Epoch 21/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4602 - val_loss: 0.4589\n",
      "Epoch 22/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4600 - val_loss: 0.4584\n",
      "Epoch 23/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4597 - val_loss: 0.4582\n",
      "Epoch 24/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4594 - val_loss: 0.4580\n",
      "Epoch 25/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4592 - val_loss: 0.4578\n",
      "Epoch 26/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4591 - val_loss: 0.4577\n",
      "Epoch 27/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4589 - val_loss: 0.4575\n",
      "Epoch 28/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4588 - val_loss: 0.4574\n",
      "Epoch 29/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4588 - val_loss: 0.4573\n",
      "Epoch 30/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4587 - val_loss: 0.4572\n",
      "Epoch 31/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4586 - val_loss: 0.4571\n",
      "Epoch 32/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4584 - val_loss: 0.4570\n",
      "Epoch 33/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4584 - val_loss: 0.4568\n",
      "Epoch 34/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4582 - val_loss: 0.4568\n",
      "Epoch 35/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4581 - val_loss: 0.4567\n",
      "Epoch 36/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4580 - val_loss: 0.4566\n",
      "Epoch 37/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4579 - val_loss: 0.4565\n",
      "Epoch 38/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4580 - val_loss: 0.4565\n",
      "Epoch 39/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4578 - val_loss: 0.4564\n",
      "Epoch 40/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4577 - val_loss: 0.4563\n",
      "Epoch 41/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4577 - val_loss: 0.4564\n",
      "Epoch 42/1500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4577 - val_loss: 0.4562\n",
      "Epoch 43/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4576 - val_loss: 0.4562\n",
      "Epoch 44/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4577 - val_loss: 0.4562\n",
      "Epoch 45/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4575 - val_loss: 0.4561\n",
      "Epoch 46/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4575 - val_loss: 0.4561\n",
      "Epoch 47/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4574 - val_loss: 0.4560\n",
      "Epoch 48/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4574 - val_loss: 0.4560\n",
      "Epoch 49/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4573 - val_loss: 0.4559\n",
      "Epoch 50/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4573 - val_loss: 0.4559\n",
      "Epoch 51/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4572 - val_loss: 0.4559\n",
      "Epoch 52/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4572 - val_loss: 0.4560\n",
      "Epoch 53/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4572 - val_loss: 0.4558\n",
      "Epoch 54/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4572 - val_loss: 0.4558\n",
      "Epoch 55/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4571 - val_loss: 0.4557\n",
      "Epoch 56/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4570 - val_loss: 0.4557\n",
      "Epoch 57/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4571 - val_loss: 0.4557\n",
      "Epoch 58/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4570 - val_loss: 0.4556\n",
      "Epoch 59/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4569 - val_loss: 0.4556\n",
      "Epoch 60/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4570 - val_loss: 0.4555\n",
      "Epoch 61/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4569 - val_loss: 0.4555\n",
      "Epoch 62/1500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4568 - val_loss: 0.4555\n",
      "Epoch 63/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4568 - val_loss: 0.4554\n",
      "Epoch 64/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4568 - val_loss: 0.4554\n",
      "Epoch 65/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4568 - val_loss: 0.4554\n",
      "Epoch 66/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4567 - val_loss: 0.4553\n",
      "Epoch 67/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4566 - val_loss: 0.4552\n",
      "Epoch 68/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4566 - val_loss: 0.4552\n",
      "Epoch 69/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4566 - val_loss: 0.4552\n",
      "Epoch 70/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4565 - val_loss: 0.4551\n",
      "Epoch 71/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4565 - val_loss: 0.4552\n",
      "Epoch 72/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4565 - val_loss: 0.4551\n",
      "Epoch 73/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4565 - val_loss: 0.4552\n",
      "Epoch 74/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4565 - val_loss: 0.4551\n",
      "Epoch 75/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4565 - val_loss: 0.4551\n",
      "Epoch 76/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4565 - val_loss: 0.4551\n",
      "Epoch 77/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4563 - val_loss: 0.4549\n",
      "Epoch 78/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4562 - val_loss: 0.4548\n",
      "Epoch 79/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4562 - val_loss: 0.4548\n",
      "Epoch 80/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4562 - val_loss: 0.4548\n",
      "Epoch 81/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4562 - val_loss: 0.4547\n",
      "Epoch 82/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4561 - val_loss: 0.4547\n",
      "Epoch 83/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4561 - val_loss: 0.4546\n",
      "Epoch 84/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4560 - val_loss: 0.4546\n",
      "Epoch 85/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4560 - val_loss: 0.4546\n",
      "Epoch 86/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4559 - val_loss: 0.4546\n",
      "Epoch 87/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4559 - val_loss: 0.4545\n",
      "Epoch 88/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4560 - val_loss: 0.4547\n",
      "Epoch 89/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4560 - val_loss: 0.4546\n",
      "Epoch 90/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4560 - val_loss: 0.4547\n",
      "Epoch 91/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4559 - val_loss: 0.4546\n",
      "Epoch 92/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4558 - val_loss: 0.4543\n",
      "Epoch 93/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4557 - val_loss: 0.4543\n",
      "Epoch 94/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4543\n",
      "Epoch 95/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4557 - val_loss: 0.4544\n",
      "Epoch 96/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4542\n",
      "Epoch 97/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4557 - val_loss: 0.4544\n",
      "Epoch 98/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4541\n",
      "Epoch 99/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4555 - val_loss: 0.4541\n",
      "Epoch 100/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4543\n",
      "Epoch 101/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4556 - val_loss: 0.4543\n",
      "Epoch 102/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4547\n",
      "Epoch 103/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4556 - val_loss: 0.4540\n",
      "Epoch 104/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4554 - val_loss: 0.4541\n",
      "Epoch 105/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4554 - val_loss: 0.4540\n",
      "Epoch 106/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4554 - val_loss: 0.4541\n",
      "Epoch 107/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4553 - val_loss: 0.4540\n",
      "Epoch 108/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4553 - val_loss: 0.4539\n",
      "Epoch 109/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4552 - val_loss: 0.4539\n",
      "Epoch 110/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4553 - val_loss: 0.4540\n",
      "Epoch 111/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4552 - val_loss: 0.4538\n",
      "Epoch 112/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4552 - val_loss: 0.4541\n",
      "Epoch 113/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4552 - val_loss: 0.4538\n",
      "Epoch 114/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4551 - val_loss: 0.4537\n",
      "Epoch 115/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4551 - val_loss: 0.4538\n",
      "Epoch 116/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4552 - val_loss: 0.4537\n",
      "Epoch 117/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4551 - val_loss: 0.4539\n",
      "Epoch 118/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4551 - val_loss: 0.4536\n",
      "Epoch 119/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4551 - val_loss: 0.4537\n",
      "Epoch 120/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4550 - val_loss: 0.4537\n",
      "Epoch 121/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 122/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 123/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4549 - val_loss: 0.4536\n",
      "Epoch 124/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 125/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4550 - val_loss: 0.4536\n",
      "Epoch 126/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4549 - val_loss: 0.4536\n",
      "Epoch 127/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4549 - val_loss: 0.4537\n",
      "Epoch 128/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4549 - val_loss: 0.4534\n",
      "Epoch 129/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4548 - val_loss: 0.4534\n",
      "Epoch 130/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4549 - val_loss: 0.4538\n",
      "Epoch 131/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4550 - val_loss: 0.4535\n",
      "Epoch 132/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4548 - val_loss: 0.4538\n",
      "Epoch 133/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4547 - val_loss: 0.4534\n",
      "Epoch 134/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4547 - val_loss: 0.4534\n",
      "Epoch 135/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4547 - val_loss: 0.4533\n",
      "Epoch 136/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4546 - val_loss: 0.4533\n",
      "Epoch 137/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4546 - val_loss: 0.4533\n",
      "Epoch 138/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4546 - val_loss: 0.4533\n",
      "Epoch 139/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4546 - val_loss: 0.4532\n",
      "Epoch 140/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4546 - val_loss: 0.4533\n",
      "Epoch 141/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4547 - val_loss: 0.4536\n",
      "Epoch 142/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4547 - val_loss: 0.4532\n",
      "Epoch 143/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4545 - val_loss: 0.4531\n",
      "Epoch 144/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4546 - val_loss: 0.4532\n",
      "Epoch 145/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4545 - val_loss: 0.4532\n",
      "Epoch 146/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4546 - val_loss: 0.4531\n",
      "Epoch 147/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4544 - val_loss: 0.4530\n",
      "Epoch 148/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4544 - val_loss: 0.4531\n",
      "Epoch 149/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4545 - val_loss: 0.4533\n",
      "Epoch 150/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4544 - val_loss: 0.4530\n",
      "Epoch 151/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4543 - val_loss: 0.4531\n",
      "Epoch 152/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4543 - val_loss: 0.4530\n",
      "Epoch 153/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4544 - val_loss: 0.4530\n",
      "Epoch 154/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4543 - val_loss: 0.4529\n",
      "Epoch 155/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4543 - val_loss: 0.4535\n",
      "Epoch 156/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4544 - val_loss: 0.4529\n",
      "Epoch 157/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4529\n",
      "Epoch 158/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4528\n",
      "Epoch 159/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4543 - val_loss: 0.4529\n",
      "Epoch 160/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4543 - val_loss: 0.4529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4528\n",
      "Epoch 162/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4542 - val_loss: 0.4531\n",
      "Epoch 163/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4543 - val_loss: 0.4528\n",
      "Epoch 164/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4544 - val_loss: 0.4530\n",
      "Epoch 165/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4528\n",
      "Epoch 166/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4542 - val_loss: 0.4528\n",
      "Epoch 167/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4541 - val_loss: 0.4527\n",
      "Epoch 168/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4541 - val_loss: 0.4527\n",
      "Epoch 169/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4541 - val_loss: 0.4526\n",
      "Epoch 170/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4541 - val_loss: 0.4528\n",
      "Epoch 171/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4530\n",
      "Epoch 172/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4534\n",
      "Epoch 173/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4541 - val_loss: 0.4526\n",
      "Epoch 174/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4540 - val_loss: 0.4526\n",
      "Epoch 175/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4539 - val_loss: 0.4525\n",
      "Epoch 176/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4541 - val_loss: 0.4527\n",
      "Epoch 177/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4543 - val_loss: 0.4528\n",
      "Epoch 178/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4542 - val_loss: 0.4525\n",
      "Epoch 179/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4539 - val_loss: 0.4527\n",
      "Epoch 180/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4539 - val_loss: 0.4527\n",
      "Epoch 181/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4540 - val_loss: 0.4524\n",
      "Epoch 182/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4538 - val_loss: 0.4525\n",
      "Epoch 183/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4539 - val_loss: 0.4524\n",
      "Epoch 184/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4539 - val_loss: 0.4524\n",
      "Epoch 185/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4538 - val_loss: 0.4524\n",
      "Epoch 186/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4538 - val_loss: 0.4524\n",
      "Epoch 187/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4537 - val_loss: 0.4523\n",
      "Epoch 188/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4537 - val_loss: 0.4525\n",
      "Epoch 189/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4539 - val_loss: 0.4523\n",
      "Epoch 190/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4539 - val_loss: 0.4523\n",
      "Epoch 191/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4537 - val_loss: 0.4525\n",
      "Epoch 192/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4536 - val_loss: 0.4523\n",
      "Epoch 193/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4536 - val_loss: 0.4522\n",
      "Epoch 194/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4536 - val_loss: 0.4525\n",
      "Epoch 195/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4538 - val_loss: 0.4524\n",
      "Epoch 196/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4537 - val_loss: 0.4523\n",
      "Epoch 197/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4536 - val_loss: 0.4526\n",
      "Epoch 198/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4538 - val_loss: 0.4523\n",
      "Epoch 199/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4537 - val_loss: 0.4522\n",
      "Epoch 200/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4536 - val_loss: 0.4525\n",
      "Epoch 201/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4535 - val_loss: 0.4523\n",
      "Epoch 202/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4536 - val_loss: 0.4522\n",
      "Epoch 203/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4535 - val_loss: 0.4522\n",
      "Epoch 204/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4535 - val_loss: 0.4522\n",
      "Epoch 205/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4535 - val_loss: 0.4524\n",
      "Epoch 206/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4537 - val_loss: 0.4529\n",
      "Epoch 207/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4536 - val_loss: 0.4522\n",
      "Epoch 208/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4534 - val_loss: 0.4522\n",
      "Epoch 209/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4537 - val_loss: 0.4520\n",
      "Epoch 210/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4534 - val_loss: 0.4520\n",
      "Epoch 211/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4533 - val_loss: 0.4520\n",
      "Epoch 212/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4533 - val_loss: 0.4527\n",
      "Epoch 213/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4534 - val_loss: 0.4522\n",
      "Epoch 214/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4533 - val_loss: 0.4526\n",
      "Epoch 215/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4535 - val_loss: 0.4522\n",
      "Epoch 216/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4534 - val_loss: 0.4519\n",
      "Epoch 217/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4533 - val_loss: 0.4521\n",
      "Epoch 218/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4534 - val_loss: 0.4520\n",
      "Epoch 219/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4532 - val_loss: 0.4518\n",
      "Epoch 220/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4532 - val_loss: 0.4521\n",
      "Epoch 221/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4533 - val_loss: 0.4520\n",
      "Epoch 222/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4531 - val_loss: 0.4519\n",
      "Epoch 223/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4531 - val_loss: 0.4518\n",
      "Epoch 224/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4531 - val_loss: 0.4518\n",
      "Epoch 225/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4531 - val_loss: 0.4519\n",
      "Epoch 226/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4531 - val_loss: 0.4520\n",
      "Epoch 227/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4531 - val_loss: 0.4518\n",
      "Epoch 228/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4530 - val_loss: 0.4518\n",
      "Epoch 229/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4530 - val_loss: 0.4518\n",
      "Epoch 230/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4531 - val_loss: 0.4518\n",
      "Epoch 231/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4533 - val_loss: 0.4518\n",
      "Epoch 232/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4532 - val_loss: 0.4517\n",
      "Epoch 233/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4530 - val_loss: 0.4519\n",
      "Epoch 234/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4530 - val_loss: 0.4520\n",
      "Epoch 235/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4530 - val_loss: 0.4516\n",
      "Epoch 236/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4528 - val_loss: 0.4518\n",
      "Epoch 237/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4528 - val_loss: 0.4516\n",
      "Epoch 238/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4531 - val_loss: 0.4517\n",
      "Epoch 239/1500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4528 - val_loss: 0.4516\n",
      "Epoch 240/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4528 - val_loss: 0.4518\n",
      "Epoch 241/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4529 - val_loss: 0.4516\n",
      "Epoch 242/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4529 - val_loss: 0.4516\n",
      "Epoch 243/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4527 - val_loss: 0.4519\n",
      "Epoch 244/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4529 - val_loss: 0.4517\n",
      "Epoch 245/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4530 - val_loss: 0.4516\n",
      "Epoch 246/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4528 - val_loss: 0.4515\n",
      "Epoch 247/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4526 - val_loss: 0.4515\n",
      "Epoch 248/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4526 - val_loss: 0.4519\n",
      "Epoch 249/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4529 - val_loss: 0.4516\n",
      "Epoch 250/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4528 - val_loss: 0.4519\n",
      "Epoch 251/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4526 - val_loss: 0.4514\n",
      "Epoch 252/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4525 - val_loss: 0.4515\n",
      "Epoch 253/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4526 - val_loss: 0.4514\n",
      "Epoch 254/1500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.4525 - val_loss: 0.4514\n",
      "Epoch 255/1500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4525 - val_loss: 0.4514\n",
      "Epoch 256/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4525 - val_loss: 0.4514\n",
      "Epoch 257/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4525 - val_loss: 0.4514\n",
      "Epoch 258/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4524 - val_loss: 0.4513\n",
      "Epoch 259/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4526 - val_loss: 0.4515\n",
      "Epoch 260/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4527 - val_loss: 0.4514\n",
      "Epoch 261/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4524 - val_loss: 0.4514\n",
      "Epoch 262/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4525 - val_loss: 0.4512\n",
      "Epoch 263/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4524 - val_loss: 0.4516\n",
      "Epoch 264/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4524 - val_loss: 0.4516\n",
      "Epoch 265/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4528 - val_loss: 0.4517\n",
      "Epoch 266/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4526 - val_loss: 0.4512\n",
      "Epoch 267/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4525 - val_loss: 0.4515\n",
      "Epoch 268/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4524 - val_loss: 0.4512\n",
      "Epoch 269/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4523 - val_loss: 0.4512\n",
      "Epoch 270/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4523 - val_loss: 0.4514\n",
      "Epoch 271/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4524 - val_loss: 0.4511\n",
      "Epoch 272/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4523 - val_loss: 0.4512\n",
      "Epoch 273/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4523 - val_loss: 0.4512\n",
      "Epoch 274/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4524 - val_loss: 0.4512\n",
      "Epoch 275/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4522 - val_loss: 0.4512\n",
      "Epoch 276/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4523 - val_loss: 0.4512\n",
      "Epoch 277/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4523 - val_loss: 0.4513\n",
      "Epoch 278/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4523 - val_loss: 0.4512\n",
      "Epoch 279/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4522 - val_loss: 0.4511\n",
      "Epoch 280/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4523 - val_loss: 0.4511\n",
      "Epoch 281/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4522 - val_loss: 0.4511\n",
      "Epoch 282/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4522 - val_loss: 0.4514\n",
      "Epoch 283/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4524 - val_loss: 0.4519\n",
      "Epoch 284/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4522 - val_loss: 0.4510\n",
      "Epoch 285/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4521 - val_loss: 0.4511\n",
      "Epoch 286/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4521 - val_loss: 0.4511\n",
      "Epoch 287/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4521 - val_loss: 0.4509\n",
      "Epoch 288/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4521 - val_loss: 0.4510\n",
      "Epoch 289/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4521 - val_loss: 0.4513\n",
      "Epoch 290/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4521 - val_loss: 0.4511\n",
      "Epoch 291/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4523 - val_loss: 0.4510\n",
      "Epoch 292/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4523 - val_loss: 0.4511\n",
      "Epoch 293/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4510\n",
      "Epoch 294/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4510\n",
      "Epoch 295/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4520 - val_loss: 0.4509\n",
      "Epoch 296/1500\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.452 - 0s 13ms/step - loss: 0.4520 - val_loss: 0.4509\n",
      "Epoch 297/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4510\n",
      "Epoch 298/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4512\n",
      "Epoch 299/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4523 - val_loss: 0.4514\n",
      "Epoch 300/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4521 - val_loss: 0.4509\n",
      "Epoch 301/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4520 - val_loss: 0.4509\n",
      "Epoch 302/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4519 - val_loss: 0.4510\n",
      "Epoch 303/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4509\n",
      "Epoch 304/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4511\n",
      "Epoch 305/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4508\n",
      "Epoch 306/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4521 - val_loss: 0.4511\n",
      "Epoch 307/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4520 - val_loss: 0.4509\n",
      "Epoch 308/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4508\n",
      "Epoch 309/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4519 - val_loss: 0.4509\n",
      "Epoch 310/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4508\n",
      "Epoch 311/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4518 - val_loss: 0.4508\n",
      "Epoch 312/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4520 - val_loss: 0.4508\n",
      "Epoch 313/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4514\n",
      "Epoch 314/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4507\n",
      "Epoch 315/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4518 - val_loss: 0.4509\n",
      "Epoch 316/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4510\n",
      "Epoch 317/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4521 - val_loss: 0.4507\n",
      "Epoch 318/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4520 - val_loss: 0.4510\n",
      "Epoch 319/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4518 - val_loss: 0.4509\n",
      "Epoch 320/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4518 - val_loss: 0.4509\n",
      "Epoch 321/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4507\n",
      "Epoch 322/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4507\n",
      "Epoch 323/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4510\n",
      "Epoch 324/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4518 - val_loss: 0.4509\n",
      "Epoch 325/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4520 - val_loss: 0.4507\n",
      "Epoch 326/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4518 - val_loss: 0.4507\n",
      "Epoch 327/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4512\n",
      "Epoch 328/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4506\n",
      "Epoch 329/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4509\n",
      "Epoch 330/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4518 - val_loss: 0.4509\n",
      "Epoch 331/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4517 - val_loss: 0.4509\n",
      "Epoch 332/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4507\n",
      "Epoch 333/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4516\n",
      "Epoch 334/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4523 - val_loss: 0.4515\n",
      "Epoch 335/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4508\n",
      "Epoch 336/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4506\n",
      "Epoch 337/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4517 - val_loss: 0.4509\n",
      "Epoch 338/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4517 - val_loss: 0.4507\n",
      "Epoch 339/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 340/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4518 - val_loss: 0.4508\n",
      "Epoch 341/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4506\n",
      "Epoch 342/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4518 - val_loss: 0.4508\n",
      "Epoch 343/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 344/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4510\n",
      "Epoch 345/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 346/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 347/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 348/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 349/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 350/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4509\n",
      "Epoch 351/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4509\n",
      "Epoch 352/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4506\n",
      "Epoch 353/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 354/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 355/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 356/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 357/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 358/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4507\n",
      "Epoch 359/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 360/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4507\n",
      "Epoch 361/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4506\n",
      "Epoch 362/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4509\n",
      "Epoch 363/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4508\n",
      "Epoch 364/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4506\n",
      "Epoch 365/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4519 - val_loss: 0.4518\n",
      "Epoch 366/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 367/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 368/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 369/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 370/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4518 - val_loss: 0.4506\n",
      "Epoch 371/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 372/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 373/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 374/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 375/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4508\n",
      "Epoch 376/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4507\n",
      "Epoch 377/1500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 0.4516 - val_loss: 0.4511\n",
      "Epoch 378/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4508\n",
      "Epoch 379/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4519 - val_loss: 0.4507\n",
      "Epoch 380/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4518 - val_loss: 0.4511\n",
      "Epoch 381/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4518 - val_loss: 0.4507\n",
      "Epoch 382/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4517 - val_loss: 0.4509\n",
      "Epoch 383/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4517 - val_loss: 0.4511\n",
      "Epoch 384/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 385/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4508\n",
      "Epoch 386/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4508\n",
      "Epoch 387/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4507\n",
      "Epoch 388/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 389/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4516 - val_loss: 0.4509\n",
      "Epoch 390/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4506\n",
      "Epoch 391/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4506\n",
      "Epoch 392/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4505\n",
      "Epoch 393/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4505\n",
      "Epoch 394/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4507\n",
      "Epoch 395/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 396/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4507\n",
      "Epoch 397/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 398/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 399/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4515 - val_loss: 0.4511\n",
      "Epoch 400/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4517 - val_loss: 0.4512\n",
      "Epoch 401/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 402/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4514 - val_loss: 0.4505\n",
      "Epoch 403/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4508\n",
      "Epoch 404/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4516 - val_loss: 0.4509\n",
      "Epoch 405/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4505\n",
      "Epoch 406/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 407/1500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 0.4514 - val_loss: 0.4511\n",
      "Epoch 408/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4505\n",
      "Epoch 409/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4505\n",
      "Epoch 410/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4506\n",
      "Epoch 411/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4505\n",
      "Epoch 412/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 413/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 414/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4507\n",
      "Epoch 415/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4505\n",
      "Epoch 416/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4508\n",
      "Epoch 417/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4505\n",
      "Epoch 418/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4509\n",
      "Epoch 419/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 420/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4507\n",
      "Epoch 421/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4506\n",
      "Epoch 422/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4511\n",
      "Epoch 423/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4516 - val_loss: 0.4507\n",
      "Epoch 424/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4513 - val_loss: 0.4507\n",
      "Epoch 425/1500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4515 - val_loss: 0.4506\n",
      "Epoch 426/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4507\n",
      "Epoch 427/1500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 0.4514 - val_loss: 0.4507\n",
      "Epoch 00427: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e009760970>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_06.fit(x = X_train, y=y_train, \\\n",
    "                validation_data=(X_val, y_val),\\\n",
    "                batch_size = 16384, validation_batch_size = 16384,\n",
    "                epochs = 1500, \\\n",
    "                callbacks=[early_stop_06],\\\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a907d7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(nn_model_06.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5e5fd",
   "metadata": {},
   "source": [
    "### d) Prediction and evaluation\n",
    "\n",
    "* Let's look at the predictions for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6aacef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_06 = (nn_model_06.predict(X_Test) > 0.55).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207732a",
   "metadata": {},
   "source": [
    "* Let's look at the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f14ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.10      0.16      6990\n",
      "           1       0.82      0.98      0.89     29040\n",
      "\n",
      "    accuracy                           0.81     36030\n",
      "   macro avg       0.67      0.54      0.53     36030\n",
      "weighted avg       0.76      0.81      0.75     36030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_Test, y_pred=z_pred_06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93daf4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  666  6324]\n",
      " [  612 28428]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true = y_Test, y_pred=z_pred_06))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d869c74",
   "metadata": {},
   "source": [
    "### Conclusions \n",
    "* I seem to be missing something fundamental here. This doesn't change the performance, which is the same as just taking a logistic regression. See below.\n",
    "* For reference, Portilla gets 97% accuracy on defaults and 88% on paying.\n",
    "* Portilla used dummy variables in the data, and got 78 features in the end.\n",
    "* Note that 80% of the data is customers that have repaid their loan, so the 82% accuracy doesn't mean much. Furthermore, the F1-score is rather poor on my models.\n",
    "* I'll go back to fixing the data. It seems like the issue comes from the categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e8dd2",
   "metadata": {},
   "source": [
    "## Test with logistic regression\n",
    "\n",
    "I'll test this with a logistic regression model.\n",
    "It apparently doesn't converge, and it gives very similar results to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b4576f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "332c7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2428c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "745de37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_lg = log_reg.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a12b3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.06      0.10      6990\n",
      "           1       0.81      0.99      0.89     29040\n",
      "\n",
      "    accuracy                           0.81     36030\n",
      "   macro avg       0.66      0.52      0.50     36030\n",
      "weighted avg       0.75      0.81      0.74     36030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_Test, y_pred=z_pred_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6ab492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  385  6605]\n",
      " [  368 28672]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true = y_Test, y_pred=z_pred_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290e2b4",
   "metadata": {},
   "source": [
    "_____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d15dd",
   "metadata": {},
   "source": [
    "## Model 07\n",
    "\n",
    "* Here I'm building another model with more features. I'm using the second data set that I made (35 features, 21/09/09).\n",
    "* It seems that the model got even worse with these additions (21/09/09 18:06)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c4af8",
   "metadata": {},
   "source": [
    "### a) Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b2550",
   "metadata": {},
   "source": [
    "* Here's the data with reduced number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a851c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "X = df_train_val.drop(labels=[\"loan_status_int\"], axis =1).values\n",
    "y = df_train_val[\"loan_status_int\"].values\n",
    "\n",
    "# Test data\n",
    "X_Test = df_test.drop(labels=\"loan_status_int\", axis =1).values\n",
    "y_Test = df_test[\"loan_status_int\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8f56f",
   "metadata": {},
   "source": [
    "* Apply train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a0bde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, X_val, y_train, y_val] = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb0e4",
   "metadata": {},
   "source": [
    "* Create the MinMaxScaler and fit to training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd06008",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = data_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7fd52",
   "metadata": {},
   "source": [
    "* Scale the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9feb1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data_scaler.transform(X_val)\n",
    "X_Test = data_scaler.transform(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3bc35d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241200, 34)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fff3f898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118800, 34)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fce68d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35219, 34)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620d87",
   "metadata": {},
   "source": [
    "### b) Model construction and callback\n",
    "\n",
    "* This time we have many more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "190db940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential\n",
    "nn_model_07 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model_07.add(Dense(units = 34, activation = \"relu\"))\n",
    "nn_model_07.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Hidden layers\n",
    "nn_model_07.add(Dense(units = 68, activation = \"relu\"))\n",
    "nn_model_07.add(Dropout(rate = 0.2))\n",
    "\n",
    "nn_model_07.add(Dense(units = 34, activation = \"relu\"))\n",
    "nn_model_07.add(Dropout(rate = 0.2))\n",
    "\n",
    "nn_model_07.add(Dense(units = 17, activation = \"relu\"))\n",
    "nn_model_07.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Output layer with sigmoid activation\n",
    "nn_model_07.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model:\n",
    "nn_model_07.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7a3a7",
   "metadata": {},
   "source": [
    "* Create EarlyStop. I've changed the patience from 50 to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73137e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_07 = EarlyStopping(monitor='val_loss', patience=25, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbcd2a",
   "metadata": {},
   "source": [
    "### c) Training\n",
    "\n",
    "* I'll skip the board for this  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3cb77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "15/15 [==============================] - 2s 78ms/step - loss: 0.6331 - val_loss: 0.5593\n",
      "Epoch 2/1500\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.5362 - val_loss: 0.5028\n",
      "Epoch 3/1500\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.5170 - val_loss: 0.4933\n",
      "Epoch 4/1500\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.5033 - val_loss: 0.4797\n",
      "Epoch 5/1500\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4957 - val_loss: 0.4755\n",
      "Epoch 6/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4893 - val_loss: 0.4716\n",
      "Epoch 7/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4851 - val_loss: 0.4685\n",
      "Epoch 8/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4818 - val_loss: 0.4668\n",
      "Epoch 9/1500\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.4794- ETA: 0s - loss: - 1s 72ms/step - loss: 0.4792 - val_loss: 0.4637\n",
      "Epoch 10/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4771 - val_loss: 0.4627\n",
      "Epoch 11/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4749 - val_loss: 0.4603\n",
      "Epoch 12/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4733 - val_loss: 0.4591\n",
      "Epoch 13/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4722 - val_loss: 0.4583\n",
      "Epoch 14/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4709 - val_loss: 0.4581\n",
      "Epoch 15/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4700 - val_loss: 0.4576\n",
      "Epoch 16/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4696 - val_loss: 0.4573\n",
      "Epoch 17/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4686 - val_loss: 0.4574\n",
      "Epoch 18/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4687 - val_loss: 0.4568\n",
      "Epoch 19/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4669 - val_loss: 0.4564\n",
      "Epoch 20/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4669 - val_loss: 0.4568\n",
      "Epoch 21/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4664 - val_loss: 0.4563\n",
      "Epoch 22/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4663 - val_loss: 0.4561\n",
      "Epoch 23/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4658 - val_loss: 0.4556\n",
      "Epoch 24/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4657 - val_loss: 0.4557\n",
      "Epoch 25/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4650 - val_loss: 0.4554\n",
      "Epoch 26/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4647 - val_loss: 0.4553\n",
      "Epoch 27/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4650 - val_loss: 0.4555\n",
      "Epoch 28/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4647 - val_loss: 0.4555\n",
      "Epoch 29/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4648 - val_loss: 0.4555\n",
      "Epoch 30/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4642 - val_loss: 0.4550\n",
      "Epoch 31/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4640 - val_loss: 0.4550\n",
      "Epoch 32/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4637 - val_loss: 0.4552\n",
      "Epoch 33/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4638 - val_loss: 0.4553\n",
      "Epoch 34/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4635 - val_loss: 0.4551\n",
      "Epoch 35/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4634 - val_loss: 0.4553\n",
      "Epoch 36/1500\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.4630 - val_loss: 0.4552\n",
      "Epoch 37/1500\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.4629 - val_loss: 0.4548\n",
      "Epoch 38/1500\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.4629 - val_loss: 0.4548\n",
      "Epoch 39/1500\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.4629 - val_loss: 0.4548\n",
      "Epoch 40/1500\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.4622 - val_loss: 0.4549\n",
      "Epoch 41/1500\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.4625 - val_loss: 0.4547\n",
      "Epoch 42/1500\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.4621 - val_loss: 0.4549\n",
      "Epoch 43/1500\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.4626 - val_loss: 0.4547\n",
      "Epoch 44/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4619 - val_loss: 0.4545\n",
      "Epoch 45/1500\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.4622 - val_loss: 0.4545\n",
      "Epoch 46/1500\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.4624 - val_loss: 0.4546\n",
      "Epoch 47/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4619 - val_loss: 0.4545\n",
      "Epoch 48/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4619 - val_loss: 0.4546\n",
      "Epoch 49/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4617 - val_loss: 0.4544\n",
      "Epoch 50/1500\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.4620 - val_loss: 0.4546\n",
      "Epoch 51/1500\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4615 - val_loss: 0.4545\n",
      "Epoch 52/1500\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4616 - val_loss: 0.4544\n",
      "Epoch 53/1500\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.4613 - val_loss: 0.4545\n",
      "Epoch 54/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4616 - val_loss: 0.4548\n",
      "Epoch 55/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4614 - val_loss: 0.4546\n",
      "Epoch 56/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4610 - val_loss: 0.4543\n",
      "Epoch 57/1500\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4613 - val_loss: 0.4544\n",
      "Epoch 58/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4612 - val_loss: 0.4542\n",
      "Epoch 59/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4610 - val_loss: 0.4544\n",
      "Epoch 60/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4608 - val_loss: 0.4544\n",
      "Epoch 61/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4603 - val_loss: 0.4544\n",
      "Epoch 62/1500\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.4609 - val_loss: 0.4543\n",
      "Epoch 63/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4609 - val_loss: 0.4544\n",
      "Epoch 64/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4607 - val_loss: 0.4542\n",
      "Epoch 65/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4602 - val_loss: 0.4550\n",
      "Epoch 66/1500\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4608 - val_loss: 0.4540\n",
      "Epoch 67/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4607 - val_loss: 0.4541\n",
      "Epoch 68/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4601 - val_loss: 0.4542\n",
      "Epoch 69/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4604 - val_loss: 0.4544\n",
      "Epoch 70/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4602 - val_loss: 0.4541\n",
      "Epoch 71/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4602 - val_loss: 0.4539\n",
      "Epoch 72/1500\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.4602 - val_loss: 0.4544\n",
      "Epoch 73/1500\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.4603 - val_loss: 0.4542\n",
      "Epoch 74/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4597 - val_loss: 0.4541\n",
      "Epoch 75/1500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4596 - val_loss: 0.4542\n",
      "Epoch 76/1500\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.4600 - val_loss: 0.4543\n",
      "Epoch 77/1500\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.4601 - val_loss: 0.4538\n",
      "Epoch 78/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4596 - val_loss: 0.4538\n",
      "Epoch 79/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4598 - val_loss: 0.4541\n",
      "Epoch 80/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4596 - val_loss: 0.4541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1500\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.4595 - val_loss: 0.4546\n",
      "Epoch 82/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4598 - val_loss: 0.4546\n",
      "Epoch 83/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4596 - val_loss: 0.4543\n",
      "Epoch 84/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4593 - val_loss: 0.4543\n",
      "Epoch 85/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4595 - val_loss: 0.4543\n",
      "Epoch 86/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4592 - val_loss: 0.4537\n",
      "Epoch 87/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4592 - val_loss: 0.4540\n",
      "Epoch 88/1500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4593 - val_loss: 0.4537\n",
      "Epoch 89/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4595 - val_loss: 0.4540\n",
      "Epoch 90/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4593 - val_loss: 0.4539\n",
      "Epoch 91/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4592 - val_loss: 0.4541\n",
      "Epoch 92/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4586 - val_loss: 0.4540\n",
      "Epoch 93/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4593 - val_loss: 0.4544\n",
      "Epoch 94/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4593 - val_loss: 0.4545\n",
      "Epoch 95/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4588 - val_loss: 0.4541\n",
      "Epoch 96/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4593 - val_loss: 0.4543\n",
      "Epoch 97/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4588 - val_loss: 0.4538\n",
      "Epoch 98/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4591 - val_loss: 0.4537\n",
      "Epoch 99/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4588 - val_loss: 0.4536\n",
      "Epoch 100/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4586 - val_loss: 0.4537\n",
      "Epoch 101/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4589 - val_loss: 0.4541\n",
      "Epoch 102/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4586 - val_loss: 0.4540\n",
      "Epoch 103/1500\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4587 - val_loss: 0.4542\n",
      "Epoch 104/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4588 - val_loss: 0.4541\n",
      "Epoch 105/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4585 - val_loss: 0.4539\n",
      "Epoch 106/1500\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.4584 - val_loss: 0.4536\n",
      "Epoch 107/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4584 - val_loss: 0.4541\n",
      "Epoch 108/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4582 - val_loss: 0.4541\n",
      "Epoch 109/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4584 - val_loss: 0.4545\n",
      "Epoch 110/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4585 - val_loss: 0.4537\n",
      "Epoch 111/1500\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4583 - val_loss: 0.4536\n",
      "Epoch 112/1500\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.4583 - val_loss: 0.4539\n",
      "Epoch 113/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4583 - val_loss: 0.4541\n",
      "Epoch 114/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4582 - val_loss: 0.4537\n",
      "Epoch 115/1500\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4579 - val_loss: 0.4543\n",
      "Epoch 116/1500\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4584 - val_loss: 0.4539\n",
      "Epoch 117/1500\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4578 - val_loss: 0.4536\n",
      "Epoch 118/1500\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.4580 - val_loss: 0.4537\n",
      "Epoch 119/1500\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4583 - val_loss: 0.4535\n",
      "Epoch 120/1500\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4580 - val_loss: 0.4537\n",
      "Epoch 121/1500\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.4580 - val_loss: 0.4537\n",
      "Epoch 122/1500\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.4578 - val_loss: 0.4538\n",
      "Epoch 123/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4581 - val_loss: 0.4545\n",
      "Epoch 124/1500\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.4577 - val_loss: 0.4537\n",
      "Epoch 125/1500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4577 - val_loss: 0.4536\n",
      "Epoch 126/1500\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.4575 - val_loss: 0.4536\n",
      "Epoch 127/1500\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.4576 - val_loss: 0.4539\n",
      "Epoch 128/1500\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.4578 - val_loss: 0.4539\n",
      "Epoch 129/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4581 - val_loss: 0.4540\n",
      "Epoch 130/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4575 - val_loss: 0.4540\n",
      "Epoch 131/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4577 - val_loss: 0.4540\n",
      "Epoch 132/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4573 - val_loss: 0.4536\n",
      "Epoch 133/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4576 - val_loss: 0.4537\n",
      "Epoch 134/1500\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.4577 - val_loss: 0.4537\n",
      "Epoch 135/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4574 - val_loss: 0.4540\n",
      "Epoch 136/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4575 - val_loss: 0.4539\n",
      "Epoch 137/1500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4573 - val_loss: 0.4533\n",
      "Epoch 138/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4574 - val_loss: 0.4536\n",
      "Epoch 139/1500\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.4575 - val_loss: 0.4535\n",
      "Epoch 140/1500\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.4570 - val_loss: 0.4538\n",
      "Epoch 141/1500\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.4573 - val_loss: 0.4537\n",
      "Epoch 142/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4575 - val_loss: 0.4538\n",
      "Epoch 143/1500\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4570 - val_loss: 0.4538\n",
      "Epoch 144/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4570 - val_loss: 0.4538\n",
      "Epoch 145/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4572 - val_loss: 0.4535\n",
      "Epoch 146/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4569 - val_loss: 0.4534\n",
      "Epoch 147/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4572 - val_loss: 0.4540\n",
      "Epoch 148/1500\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.4570 - val_loss: 0.4536\n",
      "Epoch 149/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4565 - val_loss: 0.4538\n",
      "Epoch 150/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4568 - val_loss: 0.4537\n",
      "Epoch 151/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4571 - val_loss: 0.4537\n",
      "Epoch 152/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4566 - val_loss: 0.4536\n",
      "Epoch 153/1500\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4569 - val_loss: 0.4534\n",
      "Epoch 154/1500\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4565 - val_loss: 0.4536\n",
      "Epoch 155/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4568 - val_loss: 0.4541\n",
      "Epoch 156/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4567 - val_loss: 0.4535\n",
      "Epoch 157/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4565 - val_loss: 0.4536\n",
      "Epoch 158/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4567 - val_loss: 0.4539\n",
      "Epoch 159/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4569 - val_loss: 0.4538\n",
      "Epoch 160/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4565 - val_loss: 0.4535\n",
      "Epoch 161/1500\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4566 - val_loss: 0.4534\n",
      "Epoch 162/1500\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4561 - val_loss: 0.4535\n",
      "Epoch 00162: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e931da610>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_07.fit(x = X_train, y=y_train, \\\n",
    "                validation_data=(X_val, y_val),\\\n",
    "                batch_size = 16384, validation_batch_size = 16384,\n",
    "                epochs = 1500, \\\n",
    "                callbacks=[early_stop_07],\\\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "175aab4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(nn_model_07.history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0f542",
   "metadata": {},
   "source": [
    "### d) Prediction and evaluation\n",
    "\n",
    "* Let's look at the predictions for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47418c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pred_07 = (nn_model_07.predict(X_Test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f54b0",
   "metadata": {},
   "source": [
    "* Let's look at the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa86fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.01      0.02      6978\n",
      "           1       0.80      1.00      0.89     28241\n",
      "\n",
      "    accuracy                           0.80     35219\n",
      "   macro avg       0.71      0.51      0.46     35219\n",
      "weighted avg       0.77      0.80      0.72     35219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = y_Test, y_pred=z_pred_07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "873fdfc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   88  6890]\n",
      " [   53 28188]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true = y_Test, y_pred=z_pred_07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53962c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
